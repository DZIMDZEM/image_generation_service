{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:07.317184Z",
     "start_time": "2024-05-22T08:20:07.305186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import random"
   ],
   "id": "b10883462b527219",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:04.110862Z",
     "start_time": "2024-05-22T08:20:04.095182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your dataset\n",
    "dataset_path = r'C:\\Users\\dzmit\\Downloads\\cat_images\\cats_224x224'"
   ],
   "id": "3f10b3d19164ed8c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:04.530026Z",
     "start_time": "2024-05-22T08:20:04.524860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# \n",
    "# # Path to the dataset directory you care about\n",
    "# dataset_path = r'C:\\Users\\dzmit\\Downloads\\Imageandvideodataset\\image dataset\\train\\2'\n",
    "# \n",
    "# # New subfolder path for the single class\n",
    "# subfolder_path = os.path.join(dataset_path, \"class1\")\n",
    "# \n",
    "# # Create the subfolder if it does not exist\n",
    "# if not os.path.exists(subfolder_path):\n",
    "#     os.makedirs(subfolder_path)\n",
    "# \n",
    "# # Move all files into the subfolder\n",
    "# for file in os.listdir(dataset_path):\n",
    "#     file_path = os.path.join(dataset_path, file)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         shutil.move(file_path, subfolder_path)\n",
    "# \n",
    "# print(\"All files moved to:\", subfolder_path)"
   ],
   "id": "bab98c6f91ffe48d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:09.143641Z",
     "start_time": "2024-05-22T08:20:09.043848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform for preprocessing the image\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize(256),  # Resize to 256x256\n",
    "    # transforms.CenterCrop(224),  # Crop to 224x224\n",
    "    transforms.ToTensor(),  # Convert to a tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "# dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ],
   "id": "ebdc6330df45d27b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:09.582244Z",
     "start_time": "2024-05-22T08:20:09.572247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoader\n",
    "batch_size = 8  # Batch size\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "460c215c96b56ded",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:16.492706Z",
     "start_time": "2024-05-22T08:18:15.049852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 1024, 4, 1, 0, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False),\n",
    "            \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n"
   ],
   "id": "a754ea6e6faa541a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:30.466210Z",
     "start_time": "2024-05-22T08:18:30.461623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             # input is (nc) x 64 x 64\n",
    "#             nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, input):\n",
    "#         return self.model(input).view(-1, 1).squeeze(1)\n"
   ],
   "id": "b4ec03cf704289bd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "a4e1e6a4a9dbc453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:32.634820Z",
     "start_time": "2024-05-22T08:18:31.716447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "a2c6b74b77c58978",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:34.733Z",
     "start_time": "2024-05-22T08:18:34.707433Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "e193b77c5f2fc5b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the model",
   "id": "c06399e5245aed9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:41.432424Z",
     "start_time": "2024-05-22T08:18:41.283233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = Generator().to(device)\n",
    "# discriminator = Discriminator().to(device)"
   ],
   "id": "b2f05555c8f75738",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:43.615421Z",
     "start_time": "2024-05-22T08:18:43.111630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.models import mobilenet_v2\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained MobileNet V2\n",
    "discriminator = mobilenet_v2(pretrained=True).to(device)\n",
    "\n",
    "# Freeze all the parameters for the feature extraction layers\n",
    "for param in discriminator.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier\n",
    "num_classes = 1  # Adjust based on the number of document types\n",
    "# MobileNet V2 uses 'classifier[1]' as the last layer, not 'fc'\n",
    "discriminator.classifier[1] = nn.Linear(discriminator.classifier[1].in_features, num_classes).to(device)"
   ],
   "id": "646882785df8aa5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzmit\\anaconda3\\envs\\kautra\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dzmit\\anaconda3\\envs\\kautra\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:18:45.821316Z",
     "start_time": "2024-05-22T08:18:45.809491Z"
    }
   },
   "cell_type": "code",
   "source": "discriminator(torch.randn(1, 3, 224, 224, device=device))",
   "id": "65d184e53af4329a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:06.644412Z",
     "start_time": "2024-05-22T08:19:06.633409Z"
    }
   },
   "cell_type": "code",
   "source": "# discriminator(generator(torch.randn(1, 100, 1, 1, device=device)))",
   "id": "30c52f44c77c9b62",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:24.214564Z",
     "start_time": "2024-05-22T08:19:24.196798Z"
    }
   },
   "cell_type": "code",
   "source": "generator(torch.randn(1, 100, 1, 1, device=device)).shape",
   "id": "584f5e9aa2c2943",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimizers",
   "id": "17e57702f9cb05ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:10.999682Z",
     "start_time": "2024-05-22T08:19:10.981117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ],
   "id": "79e3ca6230018c26",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the checkpoint",
   "id": "f1522e71b0d8009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:17.783152Z",
     "start_time": "2024-05-22T08:19:17.769617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checkpoint = torch.load('GAN_cat_299.pth')\n",
    "# \n",
    "# # Assuming the generator and discriminator are already instantiated as per the saved model architecture\n",
    "# generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "# discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "# \n",
    "# # Assuming the optimizers are already instantiated with the parameters of their respective models\n",
    "# optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "# optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "# \n",
    "# # If you saved the epoch number, you can also load this to know where to resume training\n",
    "# epoch = checkpoint['epoch']\n"
   ],
   "id": "45b9a74ae7638170",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:18.441509Z",
     "start_time": "2024-05-22T08:19:18.425106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for param_group in optimizer_G.param_groups:\n",
    "#     param_group['lr'] *= 0.1\n",
    "# \n",
    "# for param_group in optimizer_D.param_groups:\n",
    "#     param_group['lr'] *= 0.1"
   ],
   "id": "a2ec4a4d62a148e0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotting functions",
   "id": "b8dda8fae9cd518b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:20.749349Z",
     "start_time": "2024-05-22T08:19:20.168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_and_plot_images(n_images=9, epoch=0, plot=True):\n",
    "    \"\"\"\n",
    "    Generates and plots a grid of images using a trained generator model.\n",
    "\n",
    "    Parameters:\n",
    "    - generator: The trained generator model for generating images.\n",
    "    - device: The device (e.g., 'cuda' or 'cpu') the model should run on.\n",
    "    - n_images: The total number of images to generate and plot. Default is 9.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(9, 9))  # Create a 3x3 grid of subplots\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration\n",
    "\n",
    "    for i in range(n_images):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn(1, 100, 1, 1, device=device) / 100\n",
    "\n",
    "        # Generate an image without updating gradients\n",
    "        with torch.no_grad():\n",
    "            generated_image = generator(noise)\n",
    "\n",
    "        # Process the image for visualization\n",
    "        generated_image = generated_image.to('cpu').clone().detach()\n",
    "        generated_image = generated_image.numpy().squeeze(0)\n",
    "\n",
    "        if generated_image.shape[0] == 3:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.transpose(1, 2, 0)  # Convert from CxHxW to HxWxC\n",
    "            \n",
    "        elif generated_image.shape[0] == 1:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.squeeze(0)  # Convert from CxHxW to HxWxC\n",
    "\n",
    "        # Normalize the image data to [0, 1]\n",
    "        generated_image = (generated_image + 1) / 2\n",
    "        generated_image = generated_image.clip(0, 1)  # Ensure pixel values are within the expected range\n",
    "\n",
    "        axes[i].imshow(generated_image, cmap='gray')\n",
    "        axes[i].axis('off')  # Turn off the axis to make the images look cleaner\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/generated_images_grid_{epoch}.png')\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n"
   ],
   "id": "759d30ed7a6356a6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:15:14.917248Z",
     "start_time": "2024-05-22T08:15:14.904038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# generate_and_plot_images(n_images=25, epoch=10000)"
   ],
   "id": "e26c9972161b12e2",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clear cache",
   "id": "bb958409548b2dab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:15:16.092143Z",
     "start_time": "2024-05-22T08:15:16.087370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import gc\n",
    "# \n",
    "# torch.cuda.empty_cache()  # Clear cache\n",
    "# gc.collect()  # Collect garbage\n",
    "# generator.to('cpu')\n",
    "# discriminator.to('cpu')\n",
    "# # optimizer.to('cpu')\n",
    "# \n",
    "# del generator, discriminator, optimizer_G, optimizer_D"
   ],
   "id": "db4be72ff0c55575",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:19:43.590574Z",
     "start_time": "2024-05-22T08:19:43.574647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 300\n",
    "latent_size = 100\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "flip_prob = 0.05  # Probability of flipping labels"
   ],
   "id": "11a1971908ad53d7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:15:17.832843Z",
     "start_time": "2024-05-22T08:15:17.821841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the center crop transform\n",
    "center_crop = transforms.CenterCrop((224, 224))"
   ],
   "id": "6d33664b1e358bd1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:20:15.782013Z",
     "start_time": "2024-05-22T08:20:14.083118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    try:\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            real_images = real_images.to(device)\n",
    "            real_labels = torch.ones(real_images.size(0), device=device)\n",
    "            fake_labels = torch.zeros(real_images.size(0), device=device)\n",
    "        \n",
    "            ### Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_outputs = discriminator(real_images).squeeze(1)\n",
    "            d_loss_real = criterion(real_outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "        \n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            fake_outputs = discriminator(fake_images.detach()).squeeze(1)\n",
    "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            ### Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            # Optionally regenerate fake images for freshness\n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            output = discriminator(fake_images).squeeze(1)\n",
    "        \n",
    "            # Randomly decide whether to flip labels\n",
    "            if random.random() < flip_prob:\n",
    "                g_loss = criterion(output, fake_labels)  # Flipped labels\n",
    "            else:\n",
    "                g_loss = criterion(output, real_labels)  # Normal training\n",
    "        \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    \n",
    "            if (i + 1) % 20 == 0:\n",
    "                d_losses.append(d_loss_real.item() + d_loss_fake.item())\n",
    "                g_losses.append(g_loss.item())\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss_real.item() + d_loss_fake.item():.4f}, '\n",
    "                      f'G Loss: {g_loss.item():.4f}'\n",
    "                      f'; D Loss Real: {d_loss_real.item():.4f}, '\n",
    "                      f'; D Loss Fake: {d_loss_fake.item():.4f}, '\n",
    "                      )\n",
    "                \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            # check_output(fake_images[0], epoch)\n",
    "            generate_and_plot_images(25, epoch=epoch, plot=False)\n",
    "            \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            checkpoint = {\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch  # Optional, if you want to also save the epoch number\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'GAN_res_cat_{epoch}.pth')\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"An error occurred while processing the image. Epoch: {epoch}, batch: {i}\")\n",
    "        continue\n"
   ],
   "id": "6701c6aa62fd5790",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find a valid cuDNN algorithm to run convolution",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     g_loss \u001B[38;5;241m=\u001B[39m criterion(output, real_labels)  \u001B[38;5;66;03m# Normal training\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m \u001B[43mg_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m optimizer_G\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m20\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Unable to find a valid cuDNN algorithm to run convolution"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:15:43.034600Z",
     "start_time": "2024-05-22T08:15:43.024601Z"
    }
   },
   "cell_type": "code",
   "source": "fake_images.shape",
   "id": "9d3f48288614d29b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:16:00.810253Z",
     "start_time": "2024-05-22T08:16:00.802739Z"
    }
   },
   "cell_type": "code",
   "source": "real_images.shape",
   "id": "3a4e49c34096174b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:16:22.300947Z",
     "start_time": "2024-05-22T08:16:22.158354Z"
    }
   },
   "cell_type": "code",
   "source": "discriminator(fake_images)",
   "id": "38bdcf4173761485",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdiscriminator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfake_images\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py:174\u001B[0m, in \u001B[0;36mMobileNetV2.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py:166\u001B[0m, in \u001B[0;36mMobileNetV2._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001B[39;00m\n\u001B[1;32m--> 166\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39madaptive_avg_pool2d(x, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T08:00:12.927007Z",
     "start_time": "2024-05-22T08:00:12.920006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ],
   "id": "91b477a350795e1b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e5d66a6b98a15bb1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
