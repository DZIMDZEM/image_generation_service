{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import random"
   ],
   "id": "b10883462b527219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to your dataset\n",
    "dataset_path = r'C:\\Users\\dzmit\\Downloads\\cat_images\\cats_128x128'"
   ],
   "id": "3f10b3d19164ed8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# \n",
    "# # Path to the dataset directory you care about\n",
    "# dataset_path = r'C:\\Users\\dzmit\\Downloads\\Imageandvideodataset\\image dataset\\train\\2'\n",
    "# \n",
    "# # New subfolder path for the single class\n",
    "# subfolder_path = os.path.join(dataset_path, \"class1\")\n",
    "# \n",
    "# # Create the subfolder if it does not exist\n",
    "# if not os.path.exists(subfolder_path):\n",
    "#     os.makedirs(subfolder_path)\n",
    "# \n",
    "# # Move all files into the subfolder\n",
    "# for file in os.listdir(dataset_path):\n",
    "#     file_path = os.path.join(dataset_path, file)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         shutil.move(file_path, subfolder_path)\n",
    "# \n",
    "# print(\"All files moved to:\", subfolder_path)"
   ],
   "id": "bab98c6f91ffe48d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize all images to 64x64\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "# dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ],
   "id": "ebdc6330df45d27b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DataLoader\n",
    "batch_size = 24  # Batch size\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "460c215c96b56ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, num_layers=8):\n",
    "        super().__init__()\n",
    "        layers = [nn.Sequential(\n",
    "            nn.Linear(input_dim, feature_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Linear(feature_dim, feature_dim),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            ))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.model(z)\n",
    "        return w\n"
   ],
   "id": "9cd0e609b0d9e763",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size, w_dim, is_initial=False):\n",
    "        super().__init__()\n",
    "        if is_initial:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "            )\n",
    "        # Transform w to the correct dimension for this block\n",
    "        self.style_transform = nn.Linear(w_dim, out_channels)\n",
    "        self.styles = nn.Linear(out_channels, out_channels)\n",
    "        self.noise = nn.Parameter(torch.randn(1, 1, size, size))\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Transform style vector to match out_channels of this block\n",
    "        w_transformed = self.style_transform(w)\n",
    "        s = self.styles(w_transformed).unsqueeze(2).unsqueeze(3)\n",
    "        \n",
    "        x = x + self.noise * s\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n"
   ],
   "id": "606e44ff2a49723e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class SynthesisNetwork(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.initial_block = SynthesisBlock(512, 512, 4, w_dim=512, is_initial=True).to(device)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SynthesisBlock(512, 256, 8, w_dim=512).to(device),\n",
    "            SynthesisBlock(256, 128, 16, w_dim=512).to(device),\n",
    "            SynthesisBlock(128, 64, 32, w_dim=512).to(device),\n",
    "            SynthesisBlock(64, 3, 64, w_dim=512).to(device),\n",
    "        ])\n",
    "\n",
    "    def forward(self, w):\n",
    "        x = torch.randn(1, 512, 4, 4).to(self.device)  # Initial noise vector\n",
    "        x = self.initial_block(x, w)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, w)\n",
    "        return x\n"
   ],
   "id": "6e172ebf30736551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input).view(-1, 1).squeeze(1)\n"
   ],
   "id": "b4ec03cf704289bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debugging",
   "id": "fb1857478fa05626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# z = torch.randn(24, 512).to(device)\n",
    "# w = mapping_network(z)"
   ],
   "id": "8daf7ebe3a984b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fake_images = generator(w)\n",
    "# print(fake_images.shape)"
   ],
   "id": "d1726db47c2b6ebc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# output = discriminator(fake_images)\n",
    "# print(output.shape)"
   ],
   "id": "6b2f7345ab473b0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for i, (real_images, _) in enumerate(dataloader):\n",
    "#             real_images = real_images.to(device)\n",
    "#             real_labels = torch.ones(real_images.size(0), device=device)\n",
    "#             fake_labels = torch.zeros(real_images.size(0), device=device)\n",
    "#         \n",
    "#             ### Train Discriminator\n",
    "#             optimizer_D.zero_grad()\n",
    "#             real_outputs = discriminator(real_images)\n",
    "#             break"
   ],
   "id": "87020e2aba6edf74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# init_block = SynthesisBlock(512, 512, 4, w_dim=512, is_initial=True)\n",
    "# init_block(torch.randn(1, 512, 4, 4), torch.randn(24, 512)).shape"
   ],
   "id": "6e4d76630d104d53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# blocks = nn.ModuleList([\n",
    "#             SynthesisBlock(512, 256, 8, w_dim=512),\n",
    "#             SynthesisBlock(256, 128, 16, w_dim=512),\n",
    "#             SynthesisBlock(128, 64, 32, w_dim=512),\n",
    "#             SynthesisBlock(64, 3, 64, w_dim=512),\n",
    "#             # SynthesisBlock(32, 16, 128, w_dim=512)\n",
    "#         ])"
   ],
   "id": "148930ae57dce23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# x = torch.randn(1, 512, 4, 4)  # Initial noise vector\n",
    "# \n",
    "# x = init_block(x, w)\n",
    "# for block in blocks:\n",
    "#     x = block(x, w)\n",
    "#     print(x.shape)"
   ],
   "id": "7dd803ddd702043e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "a4e1e6a4a9dbc453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "a2c6b74b77c58978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "e193b77c5f2fc5b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the model",
   "id": "c06399e5245aed9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:44:03.861842Z",
     "start_time": "2024-05-22T09:44:03.730614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the mapping network, synthesis network, and discriminator\n",
    "mapping_network = MappingNetwork(input_dim=512, feature_dim=512).to(device)\n",
    "generator = SynthesisNetwork(device=device).to(device)\n",
    "discriminator = Discriminator().to(device)"
   ],
   "id": "f4eb9e3b02f7ee2b",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimizers",
   "id": "17e57702f9cb05ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:44:04.874676Z",
     "start_time": "2024-05-22T09:44:04.864660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(\n",
    "    list(mapping_network.parameters()) + list(generator.parameters()),\n",
    "    lr=0.001,  # Updated learning rate as per the StyleGAN paper\n",
    "    betas=(0.0, 0.99)  # Updated beta values as per the StyleGAN paper\n",
    ")\n",
    "\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=0.001,  # Updated learning rate as per the StyleGAN paper\n",
    "    betas=(0.0, 0.99)  # Updated beta values as per the StyleGAN paper\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n"
   ],
   "id": "79e3ca6230018c26",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the checkpoint",
   "id": "f1522e71b0d8009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# checkpoint = torch.load('GAN_cat_299.pth')\n",
    "# \n",
    "# # Assuming the generator and discriminator are already instantiated as per the saved model architecture\n",
    "# generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "# discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "# \n",
    "# # Assuming the optimizers are already instantiated with the parameters of their respective models\n",
    "# optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "# optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "# \n",
    "# # If you saved the epoch number, you can also load this to know where to resume training\n",
    "# epoch = checkpoint['epoch']\n"
   ],
   "id": "45b9a74ae7638170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for param_group in optimizer_G.param_groups:\n",
    "#     param_group['lr'] *= 0.1\n",
    "# \n",
    "# for param_group in optimizer_D.param_groups:\n",
    "#     param_group['lr'] *= 0.1"
   ],
   "id": "a2ec4a4d62a148e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotting functions",
   "id": "b8dda8fae9cd518b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T10:12:09.452733Z",
     "start_time": "2024-05-22T10:12:09.441307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_and_plot_images(n_images=9, epoch=0, plot=True):\n",
    "    \"\"\"\n",
    "    Generates and plots a grid of images using a trained generator model.\n",
    "\n",
    "    Parameters:\n",
    "    - generator: The trained generator model for generating images.\n",
    "    - device: The device (e.g., 'cuda' or 'cpu') the model should run on.\n",
    "    - n_images: The total number of images to generate and plot. Default is 9.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(9, 9))  # Create a 3x3 grid of subplots\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration\n",
    "\n",
    "    for i in range(n_images):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn(1, 512).to(device)\n",
    "\n",
    "        # Generate an image without updating gradients\n",
    "        with torch.no_grad():\n",
    "            generated_image = generator(mapping_network(noise))\n",
    "\n",
    "        # Process the image for visualization\n",
    "        generated_image = generated_image.to('cpu').clone().detach()\n",
    "        generated_image = generated_image.numpy().squeeze(0)\n",
    "\n",
    "        if generated_image.shape[0] == 3:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.transpose(1, 2, 0)  # Convert from CxHxW to HxWxC\n",
    "            \n",
    "        elif generated_image.shape[0] == 1:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.squeeze(0)  # Convert from CxHxW to HxWxC\n",
    "\n",
    "        # Normalize the image data to [0, 1]\n",
    "        generated_image = (generated_image + 1) / 2\n",
    "        generated_image = generated_image.clip(0, 1)  # Ensure pixel values are within the expected range\n",
    "\n",
    "        axes[i].imshow(generated_image, cmap='gray')\n",
    "        axes[i].axis('off')  # Turn off the axis to make the images look cleaner\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_base_path = 'style_net/v1/'\n",
    "    # Ensure base output directory exists\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "    plt.savefig(output_base_path + f'generated_images_grid_{epoch}.png')\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n",
    "        \n",
    "    return None\n"
   ],
   "id": "759d30ed7a6356a6",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:44:52.542843Z",
     "start_time": "2024-05-22T09:44:52.533843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_output(generated_image, epoch=0):\n",
    "    generated_image = generated_image.to('cpu').clone().detach()  # Move to CPU and detach from the computation graph\n",
    "    generated_image = generated_image.numpy()  # Convert to numpy array\n",
    "    # generated_image = generated_image.squeeze(0)  # Remove the batch dimension, resulting in (channels, height, width)\n",
    "    \n",
    "    # If the image is in the format (C, H, W), convert it to (H, W, C)\n",
    "    if generated_image.shape[0] == 3:  # Assuming 3 channels for RGB\n",
    "        generated_image = generated_image.transpose(1, 2, 0)  # Reorder dimensions to (H, W, C)\n",
    "    \n",
    "    # Normalize the image to [0, 1] if it's not already\n",
    "    generated_image = (generated_image + 1) / 2  # Assuming that the output is in the range [-1, 1]\n",
    "    generated_image = generated_image.clip(0, 1)  # Ensure the values are within [0, 1]\n",
    "    \n",
    "    plt.imshow(generated_image)\n",
    "    plt.savefig(f'output/check_generated_image_{epoch}.png')\n"
   ],
   "id": "388ab673d9ef63a8",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# generator(torch.randn(1, 512).to(device)).shape",
   "id": "880995d4c36f36ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# discriminator(generator(torch.randn(1, 512).to(device)).shape)",
   "id": "bd8fd51643ced681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# generate_and_plot_images(n_images=25, epoch=200)",
   "id": "e26c9972161b12e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clear cache",
   "id": "bb958409548b2dab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T11:19:24.933154Z",
     "start_time": "2024-05-22T11:19:21.155745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear cache\n",
    "gc.collect()  # Collect garbage\n",
    "generator.to('cpu')\n",
    "discriminator.to('cpu')\n",
    "mapping_network.to('cpu')\n",
    "\n",
    "del generator, discriminator, optimizer_G, optimizer_D"
   ],
   "id": "db4be72ff0c55575",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T09:45:44.842707Z",
     "start_time": "2024-05-22T09:45:44.833711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 300\n",
    "latent_size = 512\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "flip_prob = 0.0  # Probability of flipping labels"
   ],
   "id": "11a1971908ad53d7",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T11:18:32.403228Z",
     "start_time": "2024-05-22T10:12:19.254567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(26, num_epochs):\n",
    "    try:\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            real_images = real_images.to(device)\n",
    "            real_labels = torch.ones(real_images.size(0), device=device)\n",
    "            fake_labels = torch.zeros(real_images.size(0), device=device)\n",
    "        \n",
    "            ### Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_outputs = discriminator(real_images)\n",
    "            d_loss_real = criterion(real_outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "        \n",
    "            # Generate fake images\n",
    "            noise = torch.randn(real_images.size(0), latent_size, device=device)\n",
    "            w = mapping_network(noise)  # Transform z to w\n",
    "            fake_images = generator(w)  # Generate images from w\n",
    "            fake_outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            ### Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            # Optionally regenerate fake images for freshness\n",
    "            noise = torch.randn(real_images.size(0), latent_size, device=device)\n",
    "            w = mapping_network(noise)  # Regenerate w for generator training\n",
    "            fake_images = generator(w)\n",
    "            output = discriminator(fake_images)\n",
    "            \n",
    "        \n",
    "            # Optionally flip labels to introduce label smoothing\n",
    "            if random.random() < flip_prob:\n",
    "                g_loss = criterion(output, fake_labels)  # Flipped labels\n",
    "            else:\n",
    "                g_loss = criterion(output, real_labels)  # Normal training\n",
    "        \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            if (i + 1) % 20 == 0:\n",
    "                d_losses.append(d_loss_real.item() + d_loss_fake.item())\n",
    "                g_losses.append(g_loss.item())\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss_real.item() + d_loss_fake.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            generate_and_plot_images(25, epoch=epoch, plot=False)\n",
    "            \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            checkpoint = {\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'mapping_network_state_dict': mapping_network.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'GAN_cat_{epoch}.pth')\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"An error occurred while processing the image. Epoch: {epoch}, batch: {i}\")\n",
    "        continue\n"
   ],
   "id": "7be09657e3a2bfa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [20/417], D Loss: 0.0000, G Loss: 41.8047\n",
      "Epoch [27/300], Step [40/417], D Loss: 3.0743, G Loss: 31.6451\n",
      "Epoch [27/300], Step [60/417], D Loss: 0.0000, G Loss: 17.1550\n",
      "Epoch [27/300], Step [80/417], D Loss: 0.0002, G Loss: 19.3336\n",
      "Epoch [27/300], Step [100/417], D Loss: 0.0000, G Loss: 19.0992\n",
      "Epoch [27/300], Step [120/417], D Loss: 1.5189, G Loss: 35.0321\n",
      "Epoch [27/300], Step [140/417], D Loss: 0.1947, G Loss: 38.6341\n",
      "Epoch [27/300], Step [160/417], D Loss: 0.0000, G Loss: 38.5516\n",
      "Epoch [27/300], Step [180/417], D Loss: 0.0000, G Loss: 37.0060\n",
      "Epoch [27/300], Step [200/417], D Loss: 0.0000, G Loss: 28.0326\n",
      "Epoch [27/300], Step [220/417], D Loss: 0.0000, G Loss: 33.7269\n",
      "Epoch [27/300], Step [240/417], D Loss: 0.0000, G Loss: 31.3070\n",
      "Epoch [27/300], Step [260/417], D Loss: 0.0000, G Loss: 25.6777\n",
      "Epoch [27/300], Step [280/417], D Loss: 0.0001, G Loss: 35.2162\n",
      "Epoch [27/300], Step [300/417], D Loss: 0.0269, G Loss: 6.9806\n",
      "Epoch [27/300], Step [320/417], D Loss: 0.0115, G Loss: 46.7721\n",
      "Epoch [27/300], Step [340/417], D Loss: 0.0939, G Loss: 46.7015\n",
      "Epoch [27/300], Step [360/417], D Loss: 0.0001, G Loss: 37.2726\n",
      "Epoch [27/300], Step [380/417], D Loss: 9.8311, G Loss: 42.8916\n",
      "Epoch [27/300], Step [400/417], D Loss: 0.0029, G Loss: 6.9940\n",
      "Epoch [28/300], Step [20/417], D Loss: 0.1468, G Loss: 30.9558\n",
      "Epoch [28/300], Step [40/417], D Loss: 0.0010, G Loss: 17.7414\n",
      "Epoch [28/300], Step [60/417], D Loss: 0.4278, G Loss: 37.4397\n",
      "Epoch [28/300], Step [80/417], D Loss: 0.0000, G Loss: 22.9072\n",
      "Epoch [28/300], Step [100/417], D Loss: 0.4339, G Loss: 12.3427\n",
      "Epoch [28/300], Step [120/417], D Loss: 0.0066, G Loss: 6.7170\n",
      "Epoch [28/300], Step [140/417], D Loss: 0.0003, G Loss: 47.1252\n",
      "Epoch [28/300], Step [160/417], D Loss: 0.0004, G Loss: 47.5359\n",
      "Epoch [28/300], Step [180/417], D Loss: 0.0001, G Loss: 47.2165\n",
      "Epoch [28/300], Step [200/417], D Loss: 0.0000, G Loss: 46.6166\n",
      "Epoch [28/300], Step [220/417], D Loss: 0.0001, G Loss: 45.2253\n",
      "Epoch [28/300], Step [240/417], D Loss: 0.0001, G Loss: 45.3315\n",
      "Epoch [28/300], Step [260/417], D Loss: 0.0000, G Loss: 44.3731\n",
      "Epoch [28/300], Step [280/417], D Loss: 0.0000, G Loss: 43.9815\n",
      "Epoch [28/300], Step [300/417], D Loss: 0.0000, G Loss: 44.5198\n",
      "Epoch [28/300], Step [320/417], D Loss: 0.0000, G Loss: 44.5054\n",
      "Epoch [28/300], Step [340/417], D Loss: 0.0000, G Loss: 44.0609\n",
      "Epoch [28/300], Step [360/417], D Loss: 0.0000, G Loss: 44.2788\n",
      "Epoch [28/300], Step [380/417], D Loss: 0.0000, G Loss: 44.1705\n",
      "Epoch [28/300], Step [400/417], D Loss: 0.0000, G Loss: 41.0447\n",
      "Epoch [29/300], Step [20/417], D Loss: 0.0000, G Loss: 41.2340\n",
      "Epoch [29/300], Step [40/417], D Loss: 0.0000, G Loss: 40.8707\n",
      "Epoch [29/300], Step [60/417], D Loss: 0.0000, G Loss: 41.4595\n",
      "Epoch [29/300], Step [80/417], D Loss: 0.0000, G Loss: 41.4199\n",
      "Epoch [29/300], Step [100/417], D Loss: 0.0000, G Loss: 41.1635\n",
      "Epoch [29/300], Step [120/417], D Loss: 0.0000, G Loss: 41.3320\n",
      "Epoch [29/300], Step [140/417], D Loss: 0.0000, G Loss: 40.4884\n",
      "Epoch [29/300], Step [160/417], D Loss: 0.0000, G Loss: 40.4108\n",
      "Epoch [29/300], Step [180/417], D Loss: 0.0000, G Loss: 40.5387\n",
      "Epoch [29/300], Step [200/417], D Loss: 0.0000, G Loss: 40.6679\n",
      "Epoch [29/300], Step [220/417], D Loss: 0.0000, G Loss: 40.3581\n",
      "Epoch [29/300], Step [240/417], D Loss: 0.0000, G Loss: 40.5585\n",
      "Epoch [29/300], Step [260/417], D Loss: 0.0000, G Loss: 40.3131\n",
      "Epoch [29/300], Step [280/417], D Loss: 0.0000, G Loss: 40.3439\n",
      "Epoch [29/300], Step [300/417], D Loss: 0.0000, G Loss: 40.1594\n",
      "Epoch [29/300], Step [320/417], D Loss: 0.0000, G Loss: 40.0336\n",
      "Epoch [29/300], Step [340/417], D Loss: 0.0000, G Loss: 39.8974\n",
      "Epoch [29/300], Step [360/417], D Loss: 0.0000, G Loss: 39.7525\n",
      "Epoch [29/300], Step [380/417], D Loss: 0.0000, G Loss: 39.0036\n",
      "Epoch [29/300], Step [400/417], D Loss: 0.0000, G Loss: 39.2990\n",
      "Epoch [30/300], Step [20/417], D Loss: 0.0000, G Loss: 21.3290\n",
      "Epoch [30/300], Step [40/417], D Loss: 5.6073, G Loss: 27.6860\n",
      "Epoch [30/300], Step [60/417], D Loss: 0.0264, G Loss: 14.3255\n",
      "Epoch [30/300], Step [80/417], D Loss: 0.0123, G Loss: 10.1624\n",
      "Epoch [30/300], Step [100/417], D Loss: 0.0068, G Loss: 10.3864\n",
      "Epoch [30/300], Step [120/417], D Loss: 0.0001, G Loss: 14.2586\n",
      "Epoch [30/300], Step [140/417], D Loss: 0.0000, G Loss: 14.3514\n",
      "Epoch [30/300], Step [160/417], D Loss: 0.0028, G Loss: 8.7043\n",
      "Epoch [30/300], Step [180/417], D Loss: 0.0196, G Loss: 15.8958\n",
      "Epoch [30/300], Step [200/417], D Loss: 2.0627, G Loss: 14.3579\n",
      "Epoch [30/300], Step [220/417], D Loss: 0.0790, G Loss: 13.1597\n",
      "Epoch [30/300], Step [240/417], D Loss: 0.0976, G Loss: 10.3449\n",
      "Epoch [30/300], Step [260/417], D Loss: 0.1284, G Loss: 11.1220\n",
      "Epoch [30/300], Step [280/417], D Loss: 0.1797, G Loss: 13.5329\n",
      "Epoch [30/300], Step [300/417], D Loss: 0.5749, G Loss: 10.1314\n",
      "Epoch [30/300], Step [320/417], D Loss: 0.6034, G Loss: 22.9402\n",
      "Epoch [30/300], Step [340/417], D Loss: 0.0153, G Loss: 14.9249\n",
      "Epoch [30/300], Step [360/417], D Loss: 0.0302, G Loss: 14.8743\n",
      "Epoch [30/300], Step [380/417], D Loss: 0.2255, G Loss: 8.9271\n",
      "Epoch [30/300], Step [400/417], D Loss: 0.0028, G Loss: 16.8991\n",
      "Epoch [31/300], Step [20/417], D Loss: 0.2303, G Loss: 18.0219\n",
      "Epoch [31/300], Step [40/417], D Loss: 0.0056, G Loss: 10.2750\n",
      "Epoch [31/300], Step [60/417], D Loss: 0.0000, G Loss: 16.2557\n",
      "Epoch [31/300], Step [80/417], D Loss: 0.0239, G Loss: 9.5822\n",
      "Epoch [31/300], Step [100/417], D Loss: 0.0257, G Loss: 11.4199\n",
      "Epoch [31/300], Step [120/417], D Loss: 0.0039, G Loss: 9.7969\n",
      "Epoch [31/300], Step [140/417], D Loss: 0.0350, G Loss: 6.9161\n",
      "Epoch [31/300], Step [160/417], D Loss: 0.0146, G Loss: 6.9727\n",
      "Epoch [31/300], Step [180/417], D Loss: 0.0739, G Loss: 14.2672\n",
      "Epoch [31/300], Step [200/417], D Loss: 0.0734, G Loss: 8.8538\n",
      "Epoch [31/300], Step [220/417], D Loss: 0.0017, G Loss: 14.3786\n",
      "Epoch [31/300], Step [240/417], D Loss: 0.0025, G Loss: 7.1727\n",
      "Epoch [31/300], Step [260/417], D Loss: 5.7211, G Loss: 16.3604\n",
      "Epoch [31/300], Step [280/417], D Loss: 0.0002, G Loss: 12.5095\n",
      "Epoch [31/300], Step [300/417], D Loss: 0.0034, G Loss: 7.9445\n",
      "Epoch [31/300], Step [320/417], D Loss: 0.0004, G Loss: 12.7231\n",
      "Epoch [31/300], Step [340/417], D Loss: 0.0016, G Loss: 4.7388\n",
      "Epoch [31/300], Step [360/417], D Loss: 0.0565, G Loss: 11.6711\n",
      "Epoch [31/300], Step [380/417], D Loss: 0.8681, G Loss: 17.6184\n",
      "Epoch [31/300], Step [400/417], D Loss: 0.0161, G Loss: 17.3286\n",
      "Epoch [32/300], Step [20/417], D Loss: 0.0013, G Loss: 12.5148\n",
      "Epoch [32/300], Step [40/417], D Loss: 0.0932, G Loss: 20.9710\n",
      "Epoch [32/300], Step [60/417], D Loss: 0.0002, G Loss: 11.4962\n",
      "Epoch [32/300], Step [80/417], D Loss: 0.0335, G Loss: 10.1188\n",
      "Epoch [32/300], Step [100/417], D Loss: 0.0052, G Loss: 9.3435\n",
      "Epoch [32/300], Step [120/417], D Loss: 0.0287, G Loss: 13.9783\n",
      "Epoch [32/300], Step [140/417], D Loss: 0.0004, G Loss: 14.2942\n",
      "Epoch [32/300], Step [160/417], D Loss: 0.0395, G Loss: 7.9374\n",
      "Epoch [32/300], Step [180/417], D Loss: 0.0006, G Loss: 6.6709\n",
      "Epoch [32/300], Step [200/417], D Loss: 0.0484, G Loss: 11.1191\n",
      "Epoch [32/300], Step [220/417], D Loss: 0.0168, G Loss: 23.2879\n",
      "Epoch [32/300], Step [240/417], D Loss: 0.1531, G Loss: 22.0827\n",
      "Epoch [32/300], Step [260/417], D Loss: 0.0510, G Loss: 13.8323\n",
      "Epoch [32/300], Step [280/417], D Loss: 0.0031, G Loss: 15.3746\n",
      "Epoch [32/300], Step [300/417], D Loss: 1.1561, G Loss: 14.3379\n",
      "Epoch [32/300], Step [320/417], D Loss: 0.0000, G Loss: 7.0783\n",
      "Epoch [32/300], Step [340/417], D Loss: 0.0015, G Loss: 10.6906\n",
      "Epoch [32/300], Step [360/417], D Loss: 0.0475, G Loss: 15.1279\n",
      "Epoch [32/300], Step [380/417], D Loss: 0.0010, G Loss: 7.6368\n",
      "Epoch [32/300], Step [400/417], D Loss: 0.0028, G Loss: 7.8346\n",
      "Epoch [33/300], Step [20/417], D Loss: 0.2029, G Loss: 15.7044\n",
      "Epoch [33/300], Step [40/417], D Loss: 0.2051, G Loss: 14.8398\n",
      "Epoch [33/300], Step [60/417], D Loss: 0.0002, G Loss: 13.4347\n",
      "Epoch [33/300], Step [80/417], D Loss: 4.7915, G Loss: 20.9193\n",
      "Epoch [33/300], Step [100/417], D Loss: 0.0158, G Loss: 9.3877\n",
      "Epoch [33/300], Step [120/417], D Loss: 0.0000, G Loss: 17.9844\n",
      "Epoch [33/300], Step [140/417], D Loss: 0.0012, G Loss: 4.7382\n",
      "Epoch [33/300], Step [160/417], D Loss: 0.0107, G Loss: 5.6635\n",
      "Epoch [33/300], Step [180/417], D Loss: 0.4707, G Loss: 5.1589\n",
      "Epoch [33/300], Step [200/417], D Loss: 0.1697, G Loss: 17.0585\n",
      "Epoch [33/300], Step [220/417], D Loss: 0.0056, G Loss: 9.7914\n",
      "Epoch [33/300], Step [240/417], D Loss: 0.0044, G Loss: 8.2836\n",
      "Epoch [33/300], Step [260/417], D Loss: 0.0001, G Loss: 12.1270\n",
      "Epoch [33/300], Step [280/417], D Loss: 0.0017, G Loss: 7.9464\n",
      "Epoch [33/300], Step [300/417], D Loss: 0.0320, G Loss: 10.3958\n",
      "Epoch [33/300], Step [320/417], D Loss: 0.0007, G Loss: 9.7601\n",
      "Epoch [33/300], Step [340/417], D Loss: 0.1492, G Loss: 11.9250\n",
      "Epoch [33/300], Step [360/417], D Loss: 0.0019, G Loss: 10.4666\n",
      "Epoch [33/300], Step [380/417], D Loss: 0.0235, G Loss: 6.7363\n",
      "Epoch [33/300], Step [400/417], D Loss: 0.0465, G Loss: 15.2627\n",
      "Epoch [34/300], Step [20/417], D Loss: 0.1194, G Loss: 19.1723\n",
      "Epoch [34/300], Step [40/417], D Loss: 0.0000, G Loss: 17.7694\n",
      "Epoch [34/300], Step [60/417], D Loss: 0.0021, G Loss: 8.4142\n",
      "Epoch [34/300], Step [80/417], D Loss: 0.0000, G Loss: 12.3491\n",
      "Epoch [34/300], Step [100/417], D Loss: 0.0015, G Loss: 11.0095\n",
      "Epoch [34/300], Step [120/417], D Loss: 0.0054, G Loss: 15.2075\n",
      "Epoch [34/300], Step [140/417], D Loss: 0.0341, G Loss: 10.4278\n",
      "Epoch [34/300], Step [160/417], D Loss: 0.0019, G Loss: 7.4481\n",
      "Epoch [34/300], Step [180/417], D Loss: 0.0060, G Loss: 10.8348\n",
      "Epoch [34/300], Step [200/417], D Loss: 0.0034, G Loss: 10.1175\n",
      "Epoch [34/300], Step [220/417], D Loss: 0.0000, G Loss: 53.0579\n",
      "Epoch [34/300], Step [240/417], D Loss: 0.0000, G Loss: 51.6786\n",
      "Epoch [34/300], Step [260/417], D Loss: 0.0000, G Loss: 52.7394\n",
      "Epoch [34/300], Step [280/417], D Loss: 0.0000, G Loss: 52.5548\n",
      "Epoch [34/300], Step [300/417], D Loss: 0.0019, G Loss: 52.2248\n",
      "Epoch [34/300], Step [320/417], D Loss: 0.0000, G Loss: 52.1881\n",
      "Epoch [34/300], Step [340/417], D Loss: 0.0000, G Loss: 51.4538\n",
      "Epoch [34/300], Step [360/417], D Loss: 0.0000, G Loss: 41.7078\n",
      "Epoch [34/300], Step [380/417], D Loss: 0.0000, G Loss: 13.3552\n",
      "Epoch [34/300], Step [400/417], D Loss: 0.0112, G Loss: 6.4671\n",
      "Epoch [35/300], Step [20/417], D Loss: 0.0001, G Loss: 12.4986\n",
      "Epoch [35/300], Step [40/417], D Loss: 0.0000, G Loss: 18.9671\n",
      "Epoch [35/300], Step [60/417], D Loss: 0.0010, G Loss: 8.0062\n",
      "Epoch [35/300], Step [80/417], D Loss: 0.0005, G Loss: 10.6643\n",
      "Epoch [35/300], Step [100/417], D Loss: 0.0000, G Loss: 5.0403\n",
      "Epoch [35/300], Step [120/417], D Loss: 0.0007, G Loss: 20.7735\n",
      "Epoch [35/300], Step [140/417], D Loss: 0.0003, G Loss: 11.9046\n",
      "Epoch [35/300], Step [160/417], D Loss: 0.0013, G Loss: 11.7973\n",
      "Epoch [35/300], Step [180/417], D Loss: 0.3155, G Loss: 19.2844\n",
      "Epoch [35/300], Step [200/417], D Loss: 0.0998, G Loss: 10.7751\n",
      "Epoch [35/300], Step [220/417], D Loss: 0.3042, G Loss: 12.9834\n",
      "Epoch [35/300], Step [240/417], D Loss: 0.0170, G Loss: 7.2130\n",
      "Epoch [35/300], Step [260/417], D Loss: 0.0040, G Loss: 10.2919\n",
      "Epoch [35/300], Step [280/417], D Loss: 0.0020, G Loss: 17.9641\n",
      "Epoch [35/300], Step [300/417], D Loss: 0.0015, G Loss: 14.1274\n",
      "Epoch [35/300], Step [320/417], D Loss: 0.0052, G Loss: 19.2880\n",
      "Epoch [35/300], Step [340/417], D Loss: 0.0018, G Loss: 8.3481\n",
      "Epoch [35/300], Step [360/417], D Loss: 0.0954, G Loss: 13.7970\n",
      "Epoch [35/300], Step [380/417], D Loss: 0.0141, G Loss: 22.2933\n",
      "Epoch [35/300], Step [400/417], D Loss: 0.0006, G Loss: 11.8734\n",
      "Epoch [36/300], Step [20/417], D Loss: 0.0006, G Loss: 25.4938\n",
      "Epoch [36/300], Step [40/417], D Loss: 0.0296, G Loss: 4.3881\n",
      "Epoch [36/300], Step [60/417], D Loss: 0.0050, G Loss: 20.5308\n",
      "Epoch [36/300], Step [80/417], D Loss: 0.0072, G Loss: 10.9883\n",
      "Epoch [36/300], Step [100/417], D Loss: 0.0011, G Loss: 11.0449\n",
      "Epoch [36/300], Step [120/417], D Loss: 0.0010, G Loss: 12.8275\n",
      "Epoch [36/300], Step [140/417], D Loss: 0.0234, G Loss: 12.6034\n",
      "Epoch [36/300], Step [160/417], D Loss: 0.0099, G Loss: 8.5654\n",
      "Epoch [36/300], Step [180/417], D Loss: 0.1780, G Loss: 19.0719\n",
      "Epoch [36/300], Step [200/417], D Loss: 0.2909, G Loss: 10.5739\n",
      "Epoch [36/300], Step [220/417], D Loss: 0.0120, G Loss: 9.6972\n",
      "Epoch [36/300], Step [240/417], D Loss: 0.0000, G Loss: 12.1022\n",
      "Epoch [36/300], Step [260/417], D Loss: 0.0001, G Loss: 16.9537\n",
      "Epoch [36/300], Step [280/417], D Loss: 0.0001, G Loss: 13.0189\n",
      "Epoch [36/300], Step [300/417], D Loss: 0.0013, G Loss: 18.3883\n",
      "Epoch [36/300], Step [320/417], D Loss: 0.0020, G Loss: 7.4932\n",
      "Epoch [36/300], Step [340/417], D Loss: 0.0114, G Loss: 16.9072\n",
      "Epoch [36/300], Step [360/417], D Loss: 0.0090, G Loss: 13.9613\n",
      "Epoch [36/300], Step [380/417], D Loss: 0.0023, G Loss: 4.2127\n",
      "Epoch [36/300], Step [400/417], D Loss: 0.0059, G Loss: 11.8150\n",
      "Epoch [37/300], Step [20/417], D Loss: 0.0055, G Loss: 10.3778\n",
      "Epoch [37/300], Step [40/417], D Loss: 0.0043, G Loss: 8.1836\n",
      "Epoch [37/300], Step [60/417], D Loss: 0.0006, G Loss: 18.2991\n",
      "Epoch [37/300], Step [80/417], D Loss: 0.0056, G Loss: 6.2615\n",
      "Epoch [37/300], Step [100/417], D Loss: 0.0002, G Loss: 13.5218\n",
      "Epoch [37/300], Step [120/417], D Loss: 0.0004, G Loss: 9.9362\n",
      "Epoch [37/300], Step [140/417], D Loss: 0.1835, G Loss: 10.9712\n",
      "Epoch [37/300], Step [160/417], D Loss: 0.0014, G Loss: 11.1844\n",
      "Epoch [37/300], Step [180/417], D Loss: 0.0012, G Loss: 19.3361\n",
      "Epoch [37/300], Step [200/417], D Loss: 0.0000, G Loss: 12.8581\n",
      "Epoch [37/300], Step [220/417], D Loss: 0.0021, G Loss: 7.3258\n",
      "Epoch [37/300], Step [240/417], D Loss: 0.0071, G Loss: 14.4255\n",
      "Epoch [37/300], Step [260/417], D Loss: 0.0001, G Loss: 9.4842\n",
      "Epoch [37/300], Step [280/417], D Loss: 0.4158, G Loss: 8.7700\n",
      "Epoch [37/300], Step [300/417], D Loss: 0.0002, G Loss: 13.4559\n",
      "Epoch [37/300], Step [320/417], D Loss: 0.0028, G Loss: 5.9696\n",
      "Epoch [37/300], Step [340/417], D Loss: 0.0056, G Loss: 8.4762\n",
      "Epoch [37/300], Step [360/417], D Loss: 0.0003, G Loss: 8.2069\n",
      "Epoch [37/300], Step [380/417], D Loss: 10.2309, G Loss: 29.7093\n",
      "Epoch [37/300], Step [400/417], D Loss: 0.4008, G Loss: 12.1856\n",
      "Epoch [38/300], Step [20/417], D Loss: 0.0364, G Loss: 8.2699\n",
      "Epoch [38/300], Step [40/417], D Loss: 0.0036, G Loss: 7.1077\n",
      "Epoch [38/300], Step [60/417], D Loss: 0.0038, G Loss: 23.9043\n",
      "Epoch [38/300], Step [80/417], D Loss: 0.5274, G Loss: 25.6634\n",
      "Epoch [38/300], Step [100/417], D Loss: 0.0086, G Loss: 8.3855\n",
      "Epoch [38/300], Step [120/417], D Loss: 0.1479, G Loss: 5.4276\n",
      "Epoch [38/300], Step [140/417], D Loss: 0.0005, G Loss: 13.3163\n",
      "Epoch [38/300], Step [160/417], D Loss: 0.0004, G Loss: 11.3249\n",
      "Epoch [38/300], Step [180/417], D Loss: 0.0985, G Loss: 14.2557\n",
      "Epoch [38/300], Step [200/417], D Loss: 0.0180, G Loss: 32.5899\n",
      "Epoch [38/300], Step [220/417], D Loss: 0.0010, G Loss: 22.7389\n",
      "Epoch [38/300], Step [240/417], D Loss: 0.4266, G Loss: 32.4426\n",
      "Epoch [38/300], Step [260/417], D Loss: 1.9018, G Loss: 29.2372\n",
      "Epoch [38/300], Step [280/417], D Loss: 0.0013, G Loss: 12.9207\n",
      "Epoch [38/300], Step [300/417], D Loss: 0.0168, G Loss: 4.9632\n",
      "Epoch [38/300], Step [320/417], D Loss: 5.1494, G Loss: 20.6793\n",
      "Epoch [38/300], Step [340/417], D Loss: 0.0018, G Loss: 9.3095\n",
      "Epoch [38/300], Step [360/417], D Loss: 0.0307, G Loss: 11.3567\n",
      "Epoch [38/300], Step [380/417], D Loss: 1.2224, G Loss: 30.4785\n",
      "Epoch [38/300], Step [400/417], D Loss: 0.0002, G Loss: 20.1926\n",
      "Epoch [39/300], Step [20/417], D Loss: 0.3982, G Loss: 40.4787\n",
      "Epoch [39/300], Step [40/417], D Loss: 0.0117, G Loss: 22.9536\n",
      "Epoch [39/300], Step [60/417], D Loss: 0.0015, G Loss: 22.3181\n",
      "Epoch [39/300], Step [80/417], D Loss: 0.0001, G Loss: 10.4609\n",
      "Epoch [39/300], Step [100/417], D Loss: 2.8791, G Loss: 20.4975\n",
      "Epoch [39/300], Step [120/417], D Loss: 0.0016, G Loss: 7.7938\n",
      "Epoch [39/300], Step [140/417], D Loss: 0.0031, G Loss: 7.0028\n",
      "Epoch [39/300], Step [160/417], D Loss: 0.0000, G Loss: 18.0974\n",
      "Epoch [39/300], Step [180/417], D Loss: 0.0040, G Loss: 4.3909\n",
      "Epoch [39/300], Step [200/417], D Loss: 0.0030, G Loss: 12.6923\n",
      "Epoch [39/300], Step [220/417], D Loss: 0.4694, G Loss: 8.1495\n",
      "Epoch [39/300], Step [240/417], D Loss: 0.0007, G Loss: 15.5064\n",
      "Epoch [39/300], Step [260/417], D Loss: 0.0193, G Loss: 27.6105\n",
      "Epoch [39/300], Step [280/417], D Loss: 0.0002, G Loss: 21.9320\n",
      "Epoch [39/300], Step [300/417], D Loss: 0.0002, G Loss: 9.3382\n",
      "Epoch [39/300], Step [320/417], D Loss: 0.0005, G Loss: 12.7535\n",
      "Epoch [39/300], Step [340/417], D Loss: 0.0002, G Loss: 58.5608\n",
      "Epoch [39/300], Step [360/417], D Loss: 0.0005, G Loss: 58.2501\n",
      "Epoch [39/300], Step [380/417], D Loss: 0.0000, G Loss: 57.7939\n",
      "Epoch [39/300], Step [400/417], D Loss: 0.0000, G Loss: 57.6289\n",
      "Epoch [40/300], Step [20/417], D Loss: 0.0000, G Loss: 57.6991\n",
      "Epoch [40/300], Step [40/417], D Loss: 0.0000, G Loss: 57.4267\n",
      "Epoch [40/300], Step [60/417], D Loss: 0.0000, G Loss: 57.8333\n",
      "Epoch [40/300], Step [80/417], D Loss: 0.0000, G Loss: 57.5911\n",
      "Epoch [40/300], Step [100/417], D Loss: 0.0000, G Loss: 57.7756\n",
      "Epoch [40/300], Step [120/417], D Loss: 0.0000, G Loss: 57.6453\n",
      "Epoch [40/300], Step [140/417], D Loss: 0.0000, G Loss: 57.5939\n",
      "Epoch [40/300], Step [160/417], D Loss: 0.0000, G Loss: 57.3901\n",
      "Epoch [40/300], Step [180/417], D Loss: 0.0000, G Loss: 57.3011\n",
      "Epoch [40/300], Step [200/417], D Loss: 0.0000, G Loss: 57.4112\n",
      "Epoch [40/300], Step [220/417], D Loss: 0.0000, G Loss: 57.2740\n",
      "Epoch [40/300], Step [240/417], D Loss: 0.0000, G Loss: 57.1354\n",
      "Epoch [40/300], Step [260/417], D Loss: 0.0000, G Loss: 57.2904\n",
      "Epoch [40/300], Step [280/417], D Loss: 0.0000, G Loss: 57.2899\n",
      "Epoch [40/300], Step [300/417], D Loss: 0.0000, G Loss: 56.9746\n",
      "Epoch [40/300], Step [320/417], D Loss: 0.0000, G Loss: 56.9344\n",
      "Epoch [40/300], Step [340/417], D Loss: 0.0000, G Loss: 57.1256\n",
      "Epoch [40/300], Step [360/417], D Loss: 0.0000, G Loss: 56.8927\n",
      "Epoch [40/300], Step [380/417], D Loss: 0.0000, G Loss: 57.1736\n",
      "Epoch [40/300], Step [400/417], D Loss: 0.0000, G Loss: 57.0912\n",
      "Epoch [41/300], Step [20/417], D Loss: 0.0000, G Loss: 56.3737\n",
      "Epoch [41/300], Step [40/417], D Loss: 0.0000, G Loss: 57.0114\n",
      "Epoch [41/300], Step [60/417], D Loss: 0.0000, G Loss: 57.0471\n",
      "Epoch [41/300], Step [80/417], D Loss: 0.0000, G Loss: 56.9764\n",
      "Epoch [41/300], Step [100/417], D Loss: 0.0000, G Loss: 56.5458\n",
      "Epoch [41/300], Step [120/417], D Loss: 0.0000, G Loss: 56.4970\n",
      "Epoch [41/300], Step [140/417], D Loss: 0.0000, G Loss: 56.9785\n",
      "Epoch [41/300], Step [160/417], D Loss: 0.0000, G Loss: 56.8395\n",
      "Epoch [41/300], Step [180/417], D Loss: 0.0000, G Loss: 56.8946\n",
      "Epoch [41/300], Step [200/417], D Loss: 0.0000, G Loss: 56.8972\n",
      "Epoch [41/300], Step [220/417], D Loss: 0.0000, G Loss: 56.8526\n",
      "Epoch [41/300], Step [240/417], D Loss: 0.0000, G Loss: 56.7737\n",
      "Epoch [41/300], Step [260/417], D Loss: 0.0000, G Loss: 14.6366\n",
      "Epoch [41/300], Step [280/417], D Loss: 0.0000, G Loss: 11.2143\n",
      "Epoch [41/300], Step [300/417], D Loss: 0.0000, G Loss: 11.7680\n",
      "Epoch [41/300], Step [320/417], D Loss: 0.0001, G Loss: 15.5736\n",
      "Epoch [41/300], Step [340/417], D Loss: 0.0000, G Loss: 17.1009\n",
      "Epoch [41/300], Step [360/417], D Loss: 0.0006, G Loss: 43.6268\n",
      "Epoch [41/300], Step [380/417], D Loss: 0.0001, G Loss: 8.1329\n",
      "Epoch [41/300], Step [400/417], D Loss: 0.0113, G Loss: 11.1478\n",
      "Epoch [42/300], Step [20/417], D Loss: 0.0003, G Loss: 21.6286\n",
      "Epoch [42/300], Step [40/417], D Loss: 0.0001, G Loss: 12.8657\n",
      "Epoch [42/300], Step [60/417], D Loss: 0.0033, G Loss: 27.5459\n",
      "Epoch [42/300], Step [80/417], D Loss: 0.0000, G Loss: 23.5672\n",
      "Epoch [42/300], Step [100/417], D Loss: 0.0001, G Loss: 17.0610\n",
      "Epoch [42/300], Step [120/417], D Loss: 0.0000, G Loss: 20.3634\n",
      "Epoch [42/300], Step [140/417], D Loss: 0.0000, G Loss: 19.7284\n",
      "Epoch [42/300], Step [160/417], D Loss: 0.0000, G Loss: 17.9645\n",
      "Epoch [42/300], Step [180/417], D Loss: 0.0000, G Loss: 15.6844\n",
      "Epoch [42/300], Step [200/417], D Loss: 0.0000, G Loss: 14.3101\n",
      "Epoch [42/300], Step [220/417], D Loss: 0.0000, G Loss: 21.4577\n",
      "Epoch [42/300], Step [240/417], D Loss: 0.0000, G Loss: 16.3409\n",
      "Epoch [42/300], Step [260/417], D Loss: 0.0000, G Loss: 14.7321\n",
      "Epoch [42/300], Step [280/417], D Loss: 0.0000, G Loss: 14.3730\n",
      "Epoch [42/300], Step [300/417], D Loss: 0.0000, G Loss: 13.3507\n",
      "Epoch [42/300], Step [320/417], D Loss: 0.0000, G Loss: 16.0524\n",
      "Epoch [42/300], Step [340/417], D Loss: 0.0001, G Loss: 9.8619\n",
      "Epoch [42/300], Step [360/417], D Loss: 0.0000, G Loss: 41.8925\n",
      "Epoch [42/300], Step [380/417], D Loss: 7.9810, G Loss: 49.0149\n",
      "Epoch [42/300], Step [400/417], D Loss: 0.0002, G Loss: 32.4534\n",
      "Epoch [43/300], Step [20/417], D Loss: 0.0048, G Loss: 10.0191\n",
      "Epoch [43/300], Step [40/417], D Loss: 0.0052, G Loss: 17.4677\n",
      "Epoch [43/300], Step [60/417], D Loss: 0.0002, G Loss: 16.6730\n",
      "Epoch [43/300], Step [80/417], D Loss: 0.1377, G Loss: 13.0024\n",
      "Epoch [43/300], Step [100/417], D Loss: 0.0007, G Loss: 9.0216\n",
      "Epoch [43/300], Step [120/417], D Loss: 0.0090, G Loss: 5.0074\n",
      "Epoch [43/300], Step [140/417], D Loss: 0.0049, G Loss: 13.1015\n",
      "Epoch [43/300], Step [160/417], D Loss: 0.0051, G Loss: 16.1506\n",
      "Epoch [43/300], Step [180/417], D Loss: 0.0006, G Loss: 22.7783\n",
      "Epoch [43/300], Step [200/417], D Loss: 0.0000, G Loss: 13.3480\n",
      "Epoch [43/300], Step [220/417], D Loss: 0.0060, G Loss: 13.6023\n",
      "Epoch [43/300], Step [240/417], D Loss: 0.2986, G Loss: 15.1226\n",
      "Epoch [43/300], Step [260/417], D Loss: 0.0003, G Loss: 12.9192\n",
      "Epoch [43/300], Step [280/417], D Loss: 0.0022, G Loss: 12.4826\n",
      "Epoch [43/300], Step [300/417], D Loss: 0.1666, G Loss: 35.3427\n",
      "Epoch [43/300], Step [320/417], D Loss: 0.0000, G Loss: 10.4227\n",
      "Epoch [43/300], Step [340/417], D Loss: 0.0077, G Loss: 15.8264\n",
      "Epoch [43/300], Step [360/417], D Loss: 0.0001, G Loss: 15.8863\n",
      "Epoch [43/300], Step [380/417], D Loss: 0.0002, G Loss: 11.7057\n",
      "Epoch [43/300], Step [400/417], D Loss: 0.0000, G Loss: 18.9995\n",
      "Epoch [44/300], Step [20/417], D Loss: 0.0001, G Loss: 13.5709\n",
      "Epoch [44/300], Step [40/417], D Loss: 0.0001, G Loss: 9.1403\n",
      "Epoch [44/300], Step [60/417], D Loss: 0.0002, G Loss: 7.2372\n",
      "Epoch [44/300], Step [80/417], D Loss: 0.0022, G Loss: 7.5454\n",
      "Epoch [44/300], Step [100/417], D Loss: 0.0066, G Loss: 12.7274\n",
      "Epoch [44/300], Step [120/417], D Loss: 0.0021, G Loss: 14.5444\n",
      "Epoch [44/300], Step [140/417], D Loss: 0.0045, G Loss: 4.5813\n",
      "Epoch [44/300], Step [160/417], D Loss: 0.2908, G Loss: 6.9270\n",
      "Epoch [44/300], Step [180/417], D Loss: 0.0040, G Loss: 10.5670\n",
      "Epoch [44/300], Step [200/417], D Loss: 0.0147, G Loss: 6.5733\n",
      "Epoch [44/300], Step [220/417], D Loss: 0.0015, G Loss: 25.9093\n",
      "Epoch [44/300], Step [240/417], D Loss: 0.0000, G Loss: 29.7595\n",
      "Epoch [44/300], Step [260/417], D Loss: 1.9947, G Loss: 23.1761\n",
      "Epoch [44/300], Step [280/417], D Loss: 0.0002, G Loss: 25.4756\n",
      "Epoch [44/300], Step [300/417], D Loss: 0.0862, G Loss: 20.8551\n",
      "Epoch [44/300], Step [320/417], D Loss: 13.7759, G Loss: 29.9073\n",
      "Epoch [44/300], Step [340/417], D Loss: 0.2705, G Loss: 9.8821\n",
      "Epoch [44/300], Step [360/417], D Loss: 0.0005, G Loss: 10.3579\n",
      "Epoch [44/300], Step [380/417], D Loss: 0.8924, G Loss: 28.0870\n",
      "Epoch [44/300], Step [400/417], D Loss: 0.0009, G Loss: 22.8611\n",
      "Epoch [45/300], Step [20/417], D Loss: 0.0004, G Loss: 17.6787\n",
      "Epoch [45/300], Step [40/417], D Loss: 0.1345, G Loss: 24.9580\n",
      "Epoch [45/300], Step [60/417], D Loss: 0.0050, G Loss: 6.6427\n",
      "Epoch [45/300], Step [80/417], D Loss: 0.0318, G Loss: 9.1681\n",
      "Epoch [45/300], Step [100/417], D Loss: 0.0004, G Loss: 16.2374\n",
      "Epoch [45/300], Step [120/417], D Loss: 0.0094, G Loss: 7.9072\n",
      "Epoch [45/300], Step [140/417], D Loss: 0.0000, G Loss: 17.7491\n",
      "Epoch [45/300], Step [160/417], D Loss: 0.0206, G Loss: 12.8850\n",
      "Epoch [45/300], Step [180/417], D Loss: 0.2974, G Loss: 11.6860\n",
      "Epoch [45/300], Step [200/417], D Loss: 0.0000, G Loss: 20.2017\n",
      "Epoch [45/300], Step [220/417], D Loss: 0.0042, G Loss: 39.5971\n",
      "Epoch [45/300], Step [240/417], D Loss: 0.0001, G Loss: 46.0504\n",
      "Epoch [45/300], Step [260/417], D Loss: 0.0008, G Loss: 45.6615\n",
      "Epoch [45/300], Step [280/417], D Loss: 0.0012, G Loss: 44.8338\n",
      "Epoch [45/300], Step [300/417], D Loss: 0.0007, G Loss: 44.8086\n",
      "Epoch [45/300], Step [320/417], D Loss: 0.0002, G Loss: 31.6955\n",
      "Epoch [45/300], Step [340/417], D Loss: 0.0030, G Loss: 35.0593\n",
      "Epoch [45/300], Step [360/417], D Loss: 0.0014, G Loss: 35.7157\n",
      "Epoch [45/300], Step [380/417], D Loss: 0.0136, G Loss: 17.1631\n",
      "Epoch [45/300], Step [400/417], D Loss: 0.0317, G Loss: 18.8690\n",
      "Epoch [46/300], Step [20/417], D Loss: 0.0009, G Loss: 11.7223\n",
      "Epoch [46/300], Step [40/417], D Loss: 0.0057, G Loss: 12.5318\n",
      "Epoch [46/300], Step [60/417], D Loss: 0.8536, G Loss: 13.9554\n",
      "Epoch [46/300], Step [80/417], D Loss: 0.1044, G Loss: 23.0799\n",
      "Epoch [46/300], Step [100/417], D Loss: 0.0002, G Loss: 14.3547\n",
      "Epoch [46/300], Step [120/417], D Loss: 0.0000, G Loss: 18.3456\n",
      "Epoch [46/300], Step [140/417], D Loss: 0.0002, G Loss: 17.9884\n",
      "Epoch [46/300], Step [160/417], D Loss: 3.6898, G Loss: 36.7127\n",
      "Epoch [46/300], Step [180/417], D Loss: 0.0001, G Loss: 24.3033\n",
      "Epoch [46/300], Step [200/417], D Loss: 0.0014, G Loss: 11.1414\n",
      "Epoch [46/300], Step [220/417], D Loss: 1.1879, G Loss: 42.2549\n",
      "Epoch [46/300], Step [240/417], D Loss: 0.0037, G Loss: 33.8291\n",
      "Epoch [46/300], Step [260/417], D Loss: 0.0067, G Loss: 28.7123\n",
      "Epoch [46/300], Step [280/417], D Loss: 0.0033, G Loss: 24.2944\n",
      "Epoch [46/300], Step [300/417], D Loss: 0.0000, G Loss: 21.6902\n",
      "Epoch [46/300], Step [320/417], D Loss: 0.0930, G Loss: 4.1524\n",
      "Epoch [46/300], Step [340/417], D Loss: 0.0004, G Loss: 36.8540\n",
      "Epoch [46/300], Step [360/417], D Loss: 0.0022, G Loss: 18.8539\n",
      "Epoch [46/300], Step [380/417], D Loss: 0.0441, G Loss: 24.1801\n",
      "Epoch [46/300], Step [400/417], D Loss: 0.0051, G Loss: 6.5193\n",
      "Epoch [47/300], Step [20/417], D Loss: 0.0020, G Loss: 22.3450\n",
      "Epoch [47/300], Step [40/417], D Loss: 3.5070, G Loss: 41.9469\n",
      "Epoch [47/300], Step [60/417], D Loss: 0.3382, G Loss: 20.1501\n",
      "Epoch [47/300], Step [80/417], D Loss: 0.0666, G Loss: 20.2648\n",
      "Epoch [47/300], Step [100/417], D Loss: 0.0184, G Loss: 10.9052\n",
      "Epoch [47/300], Step [120/417], D Loss: 0.0000, G Loss: 14.2042\n",
      "Epoch [47/300], Step [140/417], D Loss: 0.0039, G Loss: 9.0509\n",
      "Epoch [47/300], Step [160/417], D Loss: 0.0019, G Loss: 19.2543\n",
      "Epoch [47/300], Step [180/417], D Loss: 0.0116, G Loss: 2.3960\n",
      "Epoch [47/300], Step [200/417], D Loss: 0.0185, G Loss: 16.7583\n",
      "Epoch [47/300], Step [220/417], D Loss: 0.0926, G Loss: 11.9453\n",
      "Epoch [47/300], Step [240/417], D Loss: 0.3101, G Loss: 1.1248\n",
      "Epoch [47/300], Step [260/417], D Loss: 0.0030, G Loss: 11.1594\n",
      "Epoch [47/300], Step [280/417], D Loss: 0.0111, G Loss: 13.8532\n",
      "Epoch [47/300], Step [300/417], D Loss: 0.0333, G Loss: 14.9098\n",
      "Epoch [47/300], Step [320/417], D Loss: 0.0001, G Loss: 22.2340\n",
      "Epoch [47/300], Step [340/417], D Loss: 0.1177, G Loss: 20.1326\n",
      "Epoch [47/300], Step [360/417], D Loss: 0.0287, G Loss: 10.9113\n",
      "Epoch [47/300], Step [380/417], D Loss: 0.0031, G Loss: 21.3671\n",
      "Epoch [47/300], Step [400/417], D Loss: 0.0005, G Loss: 14.8654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzmit\\AppData\\Local\\Temp\\ipykernel_3608\\3110015246.py:13: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axes = plt.subplots(5, 5, figsize=(9, 9))  # Create a 3x3 grid of subplots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [20/417], D Loss: 0.0016, G Loss: 23.2631\n",
      "Epoch [48/300], Step [40/417], D Loss: 0.0013, G Loss: 9.4849\n",
      "Epoch [48/300], Step [60/417], D Loss: 0.0104, G Loss: 10.6804\n",
      "Epoch [48/300], Step [80/417], D Loss: 0.0000, G Loss: 28.6086\n",
      "Epoch [48/300], Step [100/417], D Loss: 0.0000, G Loss: 17.8450\n",
      "Epoch [48/300], Step [120/417], D Loss: 0.0011, G Loss: 11.5185\n",
      "Epoch [48/300], Step [140/417], D Loss: 0.0124, G Loss: 6.7696\n",
      "Epoch [48/300], Step [160/417], D Loss: 0.2541, G Loss: 24.1531\n",
      "Epoch [48/300], Step [180/417], D Loss: 0.0044, G Loss: 9.9759\n",
      "Epoch [48/300], Step [200/417], D Loss: 0.0376, G Loss: 13.8527\n",
      "Epoch [48/300], Step [220/417], D Loss: 0.0636, G Loss: 19.3898\n",
      "Epoch [48/300], Step [240/417], D Loss: 0.0004, G Loss: 7.0103\n",
      "Epoch [48/300], Step [260/417], D Loss: 0.7238, G Loss: 12.9389\n",
      "Epoch [48/300], Step [280/417], D Loss: 0.0597, G Loss: 12.0384\n",
      "Epoch [48/300], Step [300/417], D Loss: 0.0004, G Loss: 21.6528\n",
      "Epoch [48/300], Step [320/417], D Loss: 0.0034, G Loss: 12.5058\n",
      "Epoch [48/300], Step [340/417], D Loss: 0.0128, G Loss: 14.3866\n",
      "Epoch [48/300], Step [360/417], D Loss: 0.0023, G Loss: 7.5595\n",
      "Epoch [48/300], Step [380/417], D Loss: 0.0000, G Loss: 47.8207\n",
      "Epoch [48/300], Step [400/417], D Loss: 0.0017, G Loss: 45.7235\n",
      "Epoch [49/300], Step [20/417], D Loss: 0.0195, G Loss: 13.7966\n",
      "Epoch [49/300], Step [40/417], D Loss: 0.0095, G Loss: 8.5646\n",
      "Epoch [49/300], Step [60/417], D Loss: 0.0468, G Loss: 27.5859\n",
      "Epoch [49/300], Step [80/417], D Loss: 0.0024, G Loss: 9.1077\n",
      "Epoch [49/300], Step [100/417], D Loss: 0.0049, G Loss: 11.1330\n",
      "Epoch [49/300], Step [120/417], D Loss: 0.0000, G Loss: 21.2488\n",
      "Epoch [49/300], Step [140/417], D Loss: 0.7617, G Loss: 15.9957\n",
      "Epoch [49/300], Step [160/417], D Loss: 0.0572, G Loss: 23.6487\n",
      "Epoch [49/300], Step [180/417], D Loss: 0.0014, G Loss: 22.9986\n",
      "Epoch [49/300], Step [200/417], D Loss: 0.0000, G Loss: 40.1375\n",
      "Epoch [49/300], Step [220/417], D Loss: 0.0002, G Loss: 13.9269\n",
      "Epoch [49/300], Step [240/417], D Loss: 1.1623, G Loss: 38.6657\n",
      "Epoch [49/300], Step [260/417], D Loss: 0.0001, G Loss: 20.6017\n",
      "Epoch [49/300], Step [280/417], D Loss: 0.0081, G Loss: 15.8512\n",
      "Epoch [49/300], Step [300/417], D Loss: 0.1870, G Loss: 24.0829\n",
      "Epoch [49/300], Step [320/417], D Loss: 1.6325, G Loss: 34.2278\n",
      "Epoch [49/300], Step [340/417], D Loss: 0.0138, G Loss: 8.0034\n",
      "Epoch [49/300], Step [360/417], D Loss: 0.0100, G Loss: 20.5952\n",
      "Epoch [49/300], Step [380/417], D Loss: 0.0072, G Loss: 10.6299\n",
      "Epoch [49/300], Step [400/417], D Loss: 0.0106, G Loss: 13.2462\n",
      "Epoch [50/300], Step [20/417], D Loss: 0.0002, G Loss: 19.0696\n",
      "Epoch [50/300], Step [40/417], D Loss: 0.0609, G Loss: 12.2354\n",
      "Epoch [50/300], Step [60/417], D Loss: 0.0020, G Loss: 10.4180\n",
      "Epoch [50/300], Step [80/417], D Loss: 0.0098, G Loss: 12.3696\n",
      "Epoch [50/300], Step [100/417], D Loss: 0.0011, G Loss: 9.9957\n",
      "Epoch [50/300], Step [120/417], D Loss: 0.0044, G Loss: 12.3772\n",
      "Epoch [50/300], Step [140/417], D Loss: 0.0042, G Loss: 9.1241\n",
      "Epoch [50/300], Step [160/417], D Loss: 0.1469, G Loss: 43.4419\n",
      "Epoch [50/300], Step [180/417], D Loss: 0.0014, G Loss: 29.8204\n",
      "Epoch [50/300], Step [200/417], D Loss: 0.0002, G Loss: 20.2133\n",
      "Epoch [50/300], Step [220/417], D Loss: 0.0006, G Loss: 13.6899\n",
      "Epoch [50/300], Step [240/417], D Loss: 0.0000, G Loss: 23.8975\n",
      "Epoch [50/300], Step [260/417], D Loss: 0.0284, G Loss: 19.6831\n",
      "Epoch [50/300], Step [280/417], D Loss: 0.0003, G Loss: 14.9095\n",
      "Epoch [50/300], Step [300/417], D Loss: 0.0151, G Loss: 8.6371\n",
      "Epoch [50/300], Step [320/417], D Loss: 0.0063, G Loss: 5.7221\n",
      "Epoch [50/300], Step [340/417], D Loss: 0.0148, G Loss: 20.6099\n",
      "Epoch [50/300], Step [360/417], D Loss: 2.5745, G Loss: 48.7647\n",
      "Epoch [50/300], Step [380/417], D Loss: 0.0345, G Loss: 19.3501\n",
      "Epoch [50/300], Step [400/417], D Loss: 0.0012, G Loss: 32.7873\n",
      "Epoch [51/300], Step [20/417], D Loss: 0.0094, G Loss: 6.1009\n",
      "Epoch [51/300], Step [40/417], D Loss: 0.0028, G Loss: 13.9406\n",
      "Epoch [51/300], Step [60/417], D Loss: 0.0014, G Loss: 6.8313\n",
      "Epoch [51/300], Step [80/417], D Loss: 0.0082, G Loss: 12.3418\n",
      "Epoch [51/300], Step [100/417], D Loss: 0.0090, G Loss: 9.6619\n",
      "Epoch [51/300], Step [120/417], D Loss: 0.8686, G Loss: 30.3421\n",
      "Epoch [51/300], Step [140/417], D Loss: 0.0269, G Loss: 16.7992\n",
      "Epoch [51/300], Step [160/417], D Loss: 0.2026, G Loss: 7.7895\n",
      "Epoch [51/300], Step [180/417], D Loss: 0.0020, G Loss: 7.7488\n",
      "Epoch [51/300], Step [200/417], D Loss: 0.0007, G Loss: 13.6661\n",
      "Epoch [51/300], Step [220/417], D Loss: 0.0079, G Loss: 7.6246\n",
      "Epoch [51/300], Step [240/417], D Loss: 0.0064, G Loss: 7.4871\n",
      "Epoch [51/300], Step [260/417], D Loss: 0.0084, G Loss: 22.8114\n",
      "Epoch [51/300], Step [280/417], D Loss: 0.0000, G Loss: 25.3672\n",
      "Epoch [51/300], Step [300/417], D Loss: 0.0003, G Loss: 24.0861\n",
      "Epoch [51/300], Step [320/417], D Loss: 0.0040, G Loss: 16.7311\n",
      "Epoch [51/300], Step [340/417], D Loss: 0.0001, G Loss: 42.5174\n",
      "Epoch [51/300], Step [360/417], D Loss: 0.0025, G Loss: 27.6007\n",
      "Epoch [51/300], Step [380/417], D Loss: 0.0000, G Loss: 23.6645\n",
      "Epoch [51/300], Step [400/417], D Loss: 0.0020, G Loss: 16.2897\n",
      "Epoch [52/300], Step [20/417], D Loss: 0.0002, G Loss: 15.6811\n",
      "Epoch [52/300], Step [40/417], D Loss: 0.0003, G Loss: 22.2933\n",
      "Epoch [52/300], Step [60/417], D Loss: 0.0000, G Loss: 14.2819\n",
      "Epoch [52/300], Step [80/417], D Loss: 0.0000, G Loss: 13.5471\n",
      "Epoch [52/300], Step [100/417], D Loss: 0.0002, G Loss: 11.2566\n",
      "Epoch [52/300], Step [120/417], D Loss: 0.0008, G Loss: 23.6079\n",
      "Epoch [52/300], Step [140/417], D Loss: 0.0001, G Loss: 20.1942\n",
      "Epoch [52/300], Step [160/417], D Loss: 0.0009, G Loss: 5.6915\n",
      "Epoch [52/300], Step [180/417], D Loss: 0.0000, G Loss: 39.5319\n",
      "Epoch [52/300], Step [200/417], D Loss: 0.0000, G Loss: 38.5874\n",
      "Epoch [52/300], Step [220/417], D Loss: 0.0006, G Loss: 38.9071\n",
      "Epoch [52/300], Step [240/417], D Loss: 0.0000, G Loss: 37.3706\n",
      "Epoch [52/300], Step [260/417], D Loss: 0.0034, G Loss: 20.6912\n",
      "Epoch [52/300], Step [280/417], D Loss: 0.2031, G Loss: 6.9335\n",
      "Epoch [52/300], Step [300/417], D Loss: 0.0003, G Loss: 17.1505\n",
      "Epoch [52/300], Step [320/417], D Loss: 0.0000, G Loss: 16.7270\n",
      "Epoch [52/300], Step [340/417], D Loss: 0.0003, G Loss: 18.9806\n",
      "Epoch [52/300], Step [360/417], D Loss: 0.1651, G Loss: 26.3501\n",
      "Epoch [52/300], Step [380/417], D Loss: 0.0073, G Loss: 10.3344\n",
      "Epoch [52/300], Step [400/417], D Loss: 0.1426, G Loss: 18.7750\n",
      "Epoch [53/300], Step [20/417], D Loss: 0.0055, G Loss: 10.6520\n",
      "Epoch [53/300], Step [40/417], D Loss: 0.0396, G Loss: 23.3910\n",
      "Epoch [53/300], Step [60/417], D Loss: 0.0000, G Loss: 19.2306\n",
      "Epoch [53/300], Step [80/417], D Loss: 0.0077, G Loss: 26.8498\n",
      "Epoch [53/300], Step [100/417], D Loss: 0.0000, G Loss: 24.3190\n",
      "Epoch [53/300], Step [120/417], D Loss: 0.0616, G Loss: 38.4137\n",
      "Epoch [53/300], Step [140/417], D Loss: 0.0168, G Loss: 20.2283\n",
      "Epoch [53/300], Step [160/417], D Loss: 0.0126, G Loss: 7.2918\n",
      "Epoch [53/300], Step [180/417], D Loss: 0.0000, G Loss: 13.6017\n",
      "Epoch [53/300], Step [200/417], D Loss: 0.1629, G Loss: 24.7500\n",
      "Epoch [53/300], Step [220/417], D Loss: 0.1084, G Loss: 41.2909\n",
      "Epoch [53/300], Step [240/417], D Loss: 0.0006, G Loss: 13.2659\n",
      "Epoch [53/300], Step [260/417], D Loss: 0.0016, G Loss: 30.9049\n",
      "Epoch [53/300], Step [280/417], D Loss: 0.0154, G Loss: 10.0346\n",
      "Epoch [53/300], Step [300/417], D Loss: 0.0005, G Loss: 19.2332\n",
      "Epoch [53/300], Step [320/417], D Loss: 0.0023, G Loss: 18.6380\n",
      "Epoch [53/300], Step [340/417], D Loss: 0.2455, G Loss: 32.5325\n",
      "Epoch [53/300], Step [360/417], D Loss: 0.0003, G Loss: 12.6757\n",
      "Epoch [53/300], Step [380/417], D Loss: 0.0013, G Loss: 17.2383\n",
      "Epoch [53/300], Step [400/417], D Loss: 0.0079, G Loss: 16.3571\n",
      "Epoch [54/300], Step [20/417], D Loss: 0.0273, G Loss: 10.5266\n",
      "Epoch [54/300], Step [40/417], D Loss: 0.0000, G Loss: 23.9153\n",
      "Epoch [54/300], Step [60/417], D Loss: 0.0030, G Loss: 26.9807\n",
      "Epoch [54/300], Step [80/417], D Loss: 0.0000, G Loss: 19.2574\n",
      "Epoch [54/300], Step [100/417], D Loss: 0.0912, G Loss: 24.1498\n",
      "Epoch [54/300], Step [120/417], D Loss: 0.0103, G Loss: 13.9983\n",
      "Epoch [54/300], Step [140/417], D Loss: 0.0004, G Loss: 14.1367\n",
      "Epoch [54/300], Step [160/417], D Loss: 0.0001, G Loss: 8.5722\n",
      "Epoch [54/300], Step [180/417], D Loss: 0.0193, G Loss: 20.7331\n",
      "Epoch [54/300], Step [200/417], D Loss: 0.0048, G Loss: 31.4877\n",
      "Epoch [54/300], Step [220/417], D Loss: 0.0995, G Loss: 23.0896\n",
      "Epoch [54/300], Step [240/417], D Loss: 0.0013, G Loss: 21.7383\n",
      "Epoch [54/300], Step [260/417], D Loss: 0.0173, G Loss: 24.1309\n",
      "Epoch [54/300], Step [280/417], D Loss: 0.6772, G Loss: 42.1609\n",
      "Epoch [54/300], Step [300/417], D Loss: 0.0004, G Loss: 10.5492\n",
      "Epoch [54/300], Step [320/417], D Loss: 0.0004, G Loss: 26.4142\n",
      "Epoch [54/300], Step [340/417], D Loss: 0.0226, G Loss: 7.5325\n",
      "Epoch [54/300], Step [360/417], D Loss: 0.0002, G Loss: 20.2924\n",
      "Epoch [54/300], Step [380/417], D Loss: 0.0004, G Loss: 20.7477\n",
      "Epoch [54/300], Step [400/417], D Loss: 0.0012, G Loss: 14.7240\n",
      "Epoch [55/300], Step [20/417], D Loss: 0.0001, G Loss: 26.9215\n",
      "Epoch [55/300], Step [40/417], D Loss: 0.0005, G Loss: 9.1975\n",
      "Epoch [55/300], Step [60/417], D Loss: 0.0333, G Loss: 31.7555\n",
      "Epoch [55/300], Step [80/417], D Loss: 0.0006, G Loss: 12.9031\n",
      "Epoch [55/300], Step [100/417], D Loss: 0.0000, G Loss: 18.5800\n",
      "Epoch [55/300], Step [120/417], D Loss: 0.0001, G Loss: 18.5108\n",
      "Epoch [55/300], Step [140/417], D Loss: 0.0001, G Loss: 23.0415\n",
      "Epoch [55/300], Step [160/417], D Loss: 0.0049, G Loss: 6.9397\n",
      "Epoch [55/300], Step [180/417], D Loss: 0.0008, G Loss: 7.0985\n",
      "Epoch [55/300], Step [200/417], D Loss: 0.0296, G Loss: 43.0192\n",
      "Epoch [55/300], Step [220/417], D Loss: 0.0000, G Loss: 30.5422\n",
      "Epoch [55/300], Step [240/417], D Loss: 0.0020, G Loss: 27.1272\n",
      "Epoch [55/300], Step [260/417], D Loss: 0.2577, G Loss: 23.3721\n",
      "Epoch [55/300], Step [280/417], D Loss: 0.0047, G Loss: 11.2241\n",
      "Epoch [55/300], Step [300/417], D Loss: 0.0024, G Loss: 36.6295\n",
      "Epoch [55/300], Step [320/417], D Loss: 1.0577, G Loss: 35.3895\n",
      "Epoch [55/300], Step [340/417], D Loss: 0.0051, G Loss: 22.4563\n",
      "Epoch [55/300], Step [360/417], D Loss: 0.0464, G Loss: 17.1270\n",
      "Epoch [55/300], Step [380/417], D Loss: 0.0013, G Loss: 15.8506\n",
      "Epoch [55/300], Step [400/417], D Loss: 0.0014, G Loss: 3.8653\n",
      "Epoch [56/300], Step [20/417], D Loss: 0.1325, G Loss: 21.3548\n",
      "Epoch [56/300], Step [40/417], D Loss: 0.0080, G Loss: 9.4867\n",
      "Epoch [56/300], Step [60/417], D Loss: 0.0286, G Loss: 11.5540\n",
      "Epoch [56/300], Step [80/417], D Loss: 0.0393, G Loss: 29.9359\n",
      "Epoch [56/300], Step [100/417], D Loss: 0.0173, G Loss: 23.7922\n",
      "Epoch [56/300], Step [120/417], D Loss: 0.0001, G Loss: 24.5510\n",
      "Epoch [56/300], Step [140/417], D Loss: 0.0044, G Loss: 8.7489\n",
      "Epoch [56/300], Step [160/417], D Loss: 0.0001, G Loss: 10.2158\n",
      "Epoch [56/300], Step [180/417], D Loss: 0.0045, G Loss: 10.7626\n",
      "Epoch [56/300], Step [200/417], D Loss: 0.0057, G Loss: 11.5310\n",
      "Epoch [56/300], Step [220/417], D Loss: 0.2468, G Loss: 11.5500\n",
      "Epoch [56/300], Step [240/417], D Loss: 0.0003, G Loss: 10.7085\n",
      "Epoch [56/300], Step [260/417], D Loss: 0.0127, G Loss: 11.5606\n",
      "Epoch [56/300], Step [280/417], D Loss: 0.0046, G Loss: 3.5004\n",
      "Epoch [56/300], Step [300/417], D Loss: 0.0011, G Loss: 19.9704\n",
      "Epoch [56/300], Step [320/417], D Loss: 0.0112, G Loss: 10.6788\n",
      "Epoch [56/300], Step [340/417], D Loss: 0.0004, G Loss: 8.3049\n",
      "Epoch [56/300], Step [360/417], D Loss: 0.0103, G Loss: 9.9348\n",
      "Epoch [56/300], Step [380/417], D Loss: 0.0000, G Loss: 14.5055\n",
      "Epoch [56/300], Step [400/417], D Loss: 0.0013, G Loss: 11.5065\n",
      "Epoch [57/300], Step [20/417], D Loss: 0.0044, G Loss: 20.3372\n",
      "Epoch [57/300], Step [40/417], D Loss: 0.0070, G Loss: 7.7988\n",
      "Epoch [57/300], Step [60/417], D Loss: 0.0049, G Loss: 11.5927\n",
      "Epoch [57/300], Step [80/417], D Loss: 0.0017, G Loss: 11.3158\n",
      "Epoch [57/300], Step [100/417], D Loss: 0.0001, G Loss: 20.0452\n",
      "Epoch [57/300], Step [120/417], D Loss: 0.0025, G Loss: 13.9190\n",
      "Epoch [57/300], Step [140/417], D Loss: 0.0000, G Loss: 14.3917\n",
      "Epoch [57/300], Step [160/417], D Loss: 0.0213, G Loss: 16.8620\n",
      "Epoch [57/300], Step [180/417], D Loss: 0.0294, G Loss: 13.6156\n",
      "Epoch [57/300], Step [200/417], D Loss: 0.0158, G Loss: 8.2130\n",
      "Epoch [57/300], Step [220/417], D Loss: 2.7276, G Loss: 37.8295\n",
      "Epoch [57/300], Step [240/417], D Loss: 0.1881, G Loss: 16.4590\n",
      "Epoch [57/300], Step [260/417], D Loss: 0.9067, G Loss: 29.0725\n",
      "Epoch [57/300], Step [280/417], D Loss: 0.1195, G Loss: 18.7398\n",
      "Epoch [57/300], Step [300/417], D Loss: 0.0031, G Loss: 10.7467\n",
      "Epoch [57/300], Step [320/417], D Loss: 0.0169, G Loss: 5.2555\n",
      "Epoch [57/300], Step [340/417], D Loss: 0.0007, G Loss: 10.8197\n",
      "Epoch [57/300], Step [360/417], D Loss: 0.0024, G Loss: 18.2653\n",
      "Epoch [57/300], Step [380/417], D Loss: 0.0556, G Loss: 9.7768\n",
      "Epoch [57/300], Step [400/417], D Loss: 0.0038, G Loss: 16.9984\n",
      "Epoch [58/300], Step [20/417], D Loss: 0.0001, G Loss: 18.3106\n",
      "Epoch [58/300], Step [40/417], D Loss: 0.9551, G Loss: 30.4541\n",
      "Epoch [58/300], Step [60/417], D Loss: 0.0117, G Loss: 20.7321\n",
      "Epoch [58/300], Step [80/417], D Loss: 0.0032, G Loss: 20.4148\n",
      "Epoch [58/300], Step [100/417], D Loss: 1.3306, G Loss: 16.3233\n",
      "Epoch [58/300], Step [120/417], D Loss: 0.0009, G Loss: 20.3496\n",
      "Epoch [58/300], Step [140/417], D Loss: 0.0043, G Loss: 11.9941\n",
      "Epoch [58/300], Step [160/417], D Loss: 0.0001, G Loss: 16.7309\n",
      "Epoch [58/300], Step [180/417], D Loss: 0.0001, G Loss: 19.9926\n",
      "Epoch [58/300], Step [200/417], D Loss: 0.0335, G Loss: 15.6249\n",
      "Epoch [58/300], Step [220/417], D Loss: 0.0002, G Loss: 12.1952\n",
      "Epoch [58/300], Step [240/417], D Loss: 0.0049, G Loss: 22.5908\n",
      "Epoch [58/300], Step [260/417], D Loss: 0.0011, G Loss: 12.9869\n",
      "Epoch [58/300], Step [280/417], D Loss: 0.0000, G Loss: 10.3260\n",
      "Epoch [58/300], Step [300/417], D Loss: 0.0001, G Loss: 9.7426\n",
      "Epoch [58/300], Step [320/417], D Loss: 0.0135, G Loss: 30.9285\n",
      "Epoch [58/300], Step [340/417], D Loss: 0.0001, G Loss: 29.2325\n",
      "Epoch [58/300], Step [360/417], D Loss: 0.0007, G Loss: 24.6769\n",
      "Epoch [58/300], Step [380/417], D Loss: 0.0000, G Loss: 40.5299\n",
      "Epoch [58/300], Step [400/417], D Loss: 0.0001, G Loss: 14.1957\n",
      "Epoch [59/300], Step [20/417], D Loss: 0.0020, G Loss: 10.6570\n",
      "Epoch [59/300], Step [40/417], D Loss: 0.0029, G Loss: 11.5376\n",
      "Epoch [59/300], Step [60/417], D Loss: 0.6055, G Loss: 3.2556\n",
      "Epoch [59/300], Step [80/417], D Loss: 0.0105, G Loss: 9.9581\n",
      "Epoch [59/300], Step [100/417], D Loss: 0.0177, G Loss: 7.7932\n",
      "Epoch [59/300], Step [120/417], D Loss: 0.0000, G Loss: 15.2527\n",
      "Epoch [59/300], Step [140/417], D Loss: 0.6540, G Loss: 24.1445\n",
      "Epoch [59/300], Step [160/417], D Loss: 0.0002, G Loss: 13.6047\n",
      "Epoch [59/300], Step [180/417], D Loss: 0.0018, G Loss: 15.0628\n",
      "Epoch [59/300], Step [200/417], D Loss: 0.0005, G Loss: 8.1232\n",
      "Epoch [59/300], Step [220/417], D Loss: 0.0435, G Loss: 10.9010\n",
      "Epoch [59/300], Step [240/417], D Loss: 0.0212, G Loss: 9.0563\n",
      "Epoch [59/300], Step [260/417], D Loss: 0.0055, G Loss: 15.9303\n",
      "Epoch [59/300], Step [280/417], D Loss: 0.0266, G Loss: 25.2795\n",
      "Epoch [59/300], Step [300/417], D Loss: 0.0008, G Loss: 21.9575\n",
      "Epoch [59/300], Step [320/417], D Loss: 0.0278, G Loss: 14.9639\n",
      "Epoch [59/300], Step [340/417], D Loss: 0.0036, G Loss: 21.3249\n",
      "Epoch [59/300], Step [360/417], D Loss: 0.0001, G Loss: 15.6030\n",
      "Epoch [59/300], Step [380/417], D Loss: 0.0011, G Loss: 12.6320\n",
      "Epoch [59/300], Step [400/417], D Loss: 0.0000, G Loss: 13.8275\n",
      "Epoch [60/300], Step [20/417], D Loss: 3.1601, G Loss: 11.7204\n",
      "Epoch [60/300], Step [40/417], D Loss: 0.0003, G Loss: 10.1157\n",
      "Epoch [60/300], Step [60/417], D Loss: 0.0002, G Loss: 23.6344\n",
      "Epoch [60/300], Step [80/417], D Loss: 0.0066, G Loss: 24.5541\n",
      "Epoch [60/300], Step [100/417], D Loss: 0.0010, G Loss: 5.0808\n",
      "Epoch [60/300], Step [120/417], D Loss: 0.0001, G Loss: 24.8230\n",
      "Epoch [60/300], Step [140/417], D Loss: 0.0648, G Loss: 19.3827\n",
      "Epoch [60/300], Step [160/417], D Loss: 0.0105, G Loss: 24.4372\n",
      "Epoch [60/300], Step [180/417], D Loss: 0.0000, G Loss: 17.2455\n",
      "Epoch [60/300], Step [200/417], D Loss: 9.1462, G Loss: 36.2440\n",
      "Epoch [60/300], Step [220/417], D Loss: 0.0057, G Loss: 9.2223\n",
      "Epoch [60/300], Step [240/417], D Loss: 0.5917, G Loss: 2.7849\n",
      "Epoch [60/300], Step [260/417], D Loss: 0.2170, G Loss: 15.0961\n",
      "Epoch [60/300], Step [280/417], D Loss: 3.0178, G Loss: 34.7664\n",
      "Epoch [60/300], Step [300/417], D Loss: 0.0096, G Loss: 10.2450\n",
      "Epoch [60/300], Step [320/417], D Loss: 0.0211, G Loss: 10.5666\n",
      "Epoch [60/300], Step [340/417], D Loss: 0.0004, G Loss: 16.6781\n",
      "Epoch [60/300], Step [360/417], D Loss: 0.0007, G Loss: 14.6033\n",
      "Epoch [60/300], Step [380/417], D Loss: 0.0096, G Loss: 27.6310\n",
      "Epoch [60/300], Step [400/417], D Loss: 0.0006, G Loss: 26.3449\n",
      "Epoch [61/300], Step [20/417], D Loss: 0.0007, G Loss: 19.8273\n",
      "Epoch [61/300], Step [40/417], D Loss: 0.0120, G Loss: 12.2208\n",
      "Epoch [61/300], Step [60/417], D Loss: 0.0006, G Loss: 16.2428\n",
      "Epoch [61/300], Step [80/417], D Loss: 0.0008, G Loss: 13.5379\n",
      "Epoch [61/300], Step [100/417], D Loss: 0.0131, G Loss: 9.9849\n",
      "Epoch [61/300], Step [120/417], D Loss: 0.0038, G Loss: 19.6764\n",
      "Epoch [61/300], Step [140/417], D Loss: 0.0002, G Loss: 13.3582\n",
      "Epoch [61/300], Step [160/417], D Loss: 0.0021, G Loss: 23.2135\n",
      "Epoch [61/300], Step [180/417], D Loss: 0.0269, G Loss: 19.5186\n",
      "Epoch [61/300], Step [200/417], D Loss: 0.0041, G Loss: 21.3747\n",
      "Epoch [61/300], Step [220/417], D Loss: 0.0002, G Loss: 17.3176\n",
      "Epoch [61/300], Step [240/417], D Loss: 0.0015, G Loss: 16.3235\n",
      "Epoch [61/300], Step [260/417], D Loss: 0.3096, G Loss: 17.3548\n",
      "Epoch [61/300], Step [280/417], D Loss: 0.0205, G Loss: 22.4412\n",
      "Epoch [61/300], Step [300/417], D Loss: 0.9576, G Loss: 32.9265\n",
      "Epoch [61/300], Step [320/417], D Loss: 0.0363, G Loss: 18.9836\n",
      "Epoch [61/300], Step [340/417], D Loss: 0.0003, G Loss: 9.1312\n",
      "Epoch [61/300], Step [360/417], D Loss: 0.0058, G Loss: 9.3284\n",
      "Epoch [61/300], Step [380/417], D Loss: 0.0047, G Loss: 25.3036\n",
      "Epoch [61/300], Step [400/417], D Loss: 0.0006, G Loss: 28.5727\n",
      "Epoch [62/300], Step [20/417], D Loss: 0.2022, G Loss: 28.1418\n",
      "Epoch [62/300], Step [40/417], D Loss: 0.0009, G Loss: 10.1198\n",
      "Epoch [62/300], Step [60/417], D Loss: 0.0001, G Loss: 40.0791\n",
      "Epoch [62/300], Step [80/417], D Loss: 0.0385, G Loss: 20.2176\n",
      "Epoch [62/300], Step [100/417], D Loss: 0.0041, G Loss: 13.5164\n",
      "Epoch [62/300], Step [120/417], D Loss: 4.1557, G Loss: 0.0001\n",
      "Epoch [62/300], Step [140/417], D Loss: 0.0018, G Loss: 19.3396\n",
      "Epoch [62/300], Step [160/417], D Loss: 0.0095, G Loss: 11.3411\n",
      "Epoch [62/300], Step [180/417], D Loss: 0.0174, G Loss: 8.6884\n",
      "Epoch [62/300], Step [200/417], D Loss: 0.0001, G Loss: 11.1617\n",
      "Epoch [62/300], Step [220/417], D Loss: 0.0031, G Loss: 31.3195\n",
      "Epoch [62/300], Step [240/417], D Loss: 0.0014, G Loss: 15.6715\n",
      "Epoch [62/300], Step [260/417], D Loss: 0.0223, G Loss: 13.6427\n",
      "Epoch [62/300], Step [280/417], D Loss: 0.0002, G Loss: 10.6924\n",
      "Epoch [62/300], Step [300/417], D Loss: 0.0000, G Loss: 18.4036\n",
      "Epoch [62/300], Step [320/417], D Loss: 0.0000, G Loss: 32.7375\n",
      "Epoch [62/300], Step [340/417], D Loss: 0.0031, G Loss: 25.2398\n",
      "Epoch [62/300], Step [360/417], D Loss: 0.0004, G Loss: 41.1385\n",
      "Epoch [62/300], Step [380/417], D Loss: 0.0001, G Loss: 29.1905\n",
      "Epoch [62/300], Step [400/417], D Loss: 0.0013, G Loss: 53.5866\n",
      "Epoch [63/300], Step [20/417], D Loss: 0.0000, G Loss: 62.7983\n",
      "Epoch [63/300], Step [40/417], D Loss: 0.0011, G Loss: 62.7115\n",
      "Epoch [63/300], Step [60/417], D Loss: 0.0000, G Loss: 52.9386\n",
      "Epoch [63/300], Step [80/417], D Loss: 0.0000, G Loss: 62.5186\n",
      "Epoch [63/300], Step [100/417], D Loss: 0.0001, G Loss: 62.6466\n",
      "Epoch [63/300], Step [120/417], D Loss: 0.0001, G Loss: 62.7569\n",
      "Epoch [63/300], Step [140/417], D Loss: 0.0000, G Loss: 63.0975\n",
      "Epoch [63/300], Step [160/417], D Loss: 0.0000, G Loss: 56.7185\n",
      "Epoch [63/300], Step [180/417], D Loss: 0.0000, G Loss: 62.5385\n",
      "Epoch [63/300], Step [200/417], D Loss: 0.0000, G Loss: 53.4042\n",
      "Epoch [63/300], Step [220/417], D Loss: 0.0001, G Loss: 62.6092\n",
      "Epoch [63/300], Step [240/417], D Loss: 0.0000, G Loss: 52.1786\n",
      "Epoch [63/300], Step [260/417], D Loss: 0.0000, G Loss: 50.0522\n",
      "Epoch [63/300], Step [280/417], D Loss: 0.0000, G Loss: 45.7473\n",
      "Epoch [63/300], Step [300/417], D Loss: 0.0009, G Loss: 61.3862\n",
      "Epoch [63/300], Step [320/417], D Loss: 0.0000, G Loss: 46.3217\n",
      "Epoch [63/300], Step [340/417], D Loss: 0.0001, G Loss: 60.8861\n",
      "Epoch [63/300], Step [360/417], D Loss: 0.0000, G Loss: 61.7586\n",
      "Epoch [63/300], Step [380/417], D Loss: 0.0000, G Loss: 61.7301\n",
      "Epoch [63/300], Step [400/417], D Loss: 0.0000, G Loss: 51.0089\n",
      "Epoch [64/300], Step [20/417], D Loss: 0.0000, G Loss: 59.9146\n",
      "Epoch [64/300], Step [40/417], D Loss: 0.0000, G Loss: 55.8664\n",
      "Epoch [64/300], Step [60/417], D Loss: 0.0000, G Loss: 61.8930\n",
      "Epoch [64/300], Step [80/417], D Loss: 0.0000, G Loss: 33.3632\n",
      "Epoch [64/300], Step [100/417], D Loss: 0.0001, G Loss: 35.7352\n",
      "Epoch [64/300], Step [120/417], D Loss: 0.0000, G Loss: 22.8581\n",
      "Epoch [64/300], Step [140/417], D Loss: 0.0190, G Loss: 20.6269\n",
      "Epoch [64/300], Step [160/417], D Loss: 0.0009, G Loss: 21.8262\n",
      "Epoch [64/300], Step [180/417], D Loss: 0.0000, G Loss: 45.9351\n",
      "Epoch [64/300], Step [200/417], D Loss: 0.0000, G Loss: 46.0557\n",
      "Epoch [64/300], Step [220/417], D Loss: 0.0000, G Loss: 45.9957\n",
      "Epoch [64/300], Step [240/417], D Loss: 0.0000, G Loss: 46.4310\n",
      "Epoch [64/300], Step [260/417], D Loss: 0.0000, G Loss: 45.8149\n",
      "Epoch [64/300], Step [280/417], D Loss: 0.0000, G Loss: 45.7192\n",
      "Epoch [64/300], Step [300/417], D Loss: 0.0000, G Loss: 46.4843\n",
      "Epoch [64/300], Step [320/417], D Loss: 0.0000, G Loss: 45.9507\n",
      "Epoch [64/300], Step [340/417], D Loss: 0.0000, G Loss: 46.4075\n",
      "Epoch [64/300], Step [360/417], D Loss: 0.0000, G Loss: 46.0627\n",
      "Epoch [64/300], Step [380/417], D Loss: 0.0000, G Loss: 46.1968\n",
      "Epoch [64/300], Step [400/417], D Loss: 0.0000, G Loss: 46.1347\n",
      "Epoch [65/300], Step [20/417], D Loss: 0.0000, G Loss: 46.3735\n",
      "Epoch [65/300], Step [40/417], D Loss: 0.0000, G Loss: 46.1089\n",
      "Epoch [65/300], Step [60/417], D Loss: 0.0000, G Loss: 45.8567\n",
      "Epoch [65/300], Step [80/417], D Loss: 0.0000, G Loss: 45.1605\n",
      "Epoch [65/300], Step [100/417], D Loss: 0.0000, G Loss: 46.1034\n",
      "Epoch [65/300], Step [120/417], D Loss: 0.0000, G Loss: 46.5261\n",
      "Epoch [65/300], Step [140/417], D Loss: 0.0000, G Loss: 45.7501\n",
      "Epoch [65/300], Step [160/417], D Loss: 0.0000, G Loss: 45.4252\n",
      "Epoch [65/300], Step [180/417], D Loss: 0.0000, G Loss: 45.9222\n",
      "Epoch [65/300], Step [200/417], D Loss: 0.0000, G Loss: 45.3908\n",
      "Epoch [65/300], Step [220/417], D Loss: 0.0000, G Loss: 46.3067\n",
      "Epoch [65/300], Step [240/417], D Loss: 0.0000, G Loss: 46.2834\n",
      "Epoch [65/300], Step [260/417], D Loss: 0.0000, G Loss: 46.0066\n",
      "Epoch [65/300], Step [280/417], D Loss: 0.0000, G Loss: 46.0129\n",
      "Epoch [65/300], Step [300/417], D Loss: 0.0000, G Loss: 46.0368\n",
      "Epoch [65/300], Step [320/417], D Loss: 0.0000, G Loss: 45.9433\n",
      "Epoch [65/300], Step [340/417], D Loss: 0.0000, G Loss: 46.0786\n",
      "Epoch [65/300], Step [360/417], D Loss: 0.0000, G Loss: 45.9462\n",
      "Epoch [65/300], Step [380/417], D Loss: 0.0000, G Loss: 45.2046\n",
      "Epoch [65/300], Step [400/417], D Loss: 0.0000, G Loss: 46.4503\n",
      "Epoch [66/300], Step [20/417], D Loss: 0.0000, G Loss: 45.8670\n",
      "Epoch [66/300], Step [40/417], D Loss: 0.0000, G Loss: 46.2671\n",
      "Epoch [66/300], Step [60/417], D Loss: 0.0000, G Loss: 45.7279\n",
      "Epoch [66/300], Step [80/417], D Loss: 0.0000, G Loss: 46.2584\n",
      "Epoch [66/300], Step [100/417], D Loss: 0.0000, G Loss: 45.9768\n",
      "Epoch [66/300], Step [120/417], D Loss: 0.0000, G Loss: 45.7033\n",
      "Epoch [66/300], Step [140/417], D Loss: 0.0000, G Loss: 45.9796\n",
      "Epoch [66/300], Step [160/417], D Loss: 0.0000, G Loss: 45.9521\n",
      "Epoch [66/300], Step [180/417], D Loss: 0.0000, G Loss: 45.8838\n",
      "Epoch [66/300], Step [200/417], D Loss: 0.0000, G Loss: 45.8809\n",
      "Epoch [66/300], Step [220/417], D Loss: 0.0000, G Loss: 46.0461\n",
      "Epoch [66/300], Step [240/417], D Loss: 0.0000, G Loss: 45.5272\n",
      "Epoch [66/300], Step [260/417], D Loss: 0.0000, G Loss: 45.6794\n",
      "Epoch [66/300], Step [280/417], D Loss: 0.0000, G Loss: 45.1806\n",
      "Epoch [66/300], Step [300/417], D Loss: 0.0000, G Loss: 45.5538\n",
      "Epoch [66/300], Step [320/417], D Loss: 0.0000, G Loss: 45.7597\n",
      "Epoch [66/300], Step [340/417], D Loss: 0.0000, G Loss: 45.5102\n",
      "Epoch [66/300], Step [360/417], D Loss: 0.0000, G Loss: 45.7204\n",
      "Epoch [66/300], Step [380/417], D Loss: 0.0000, G Loss: 45.3893\n",
      "Epoch [66/300], Step [400/417], D Loss: 0.0000, G Loss: 45.4237\n",
      "Epoch [67/300], Step [20/417], D Loss: 0.0000, G Loss: 37.6661\n",
      "Epoch [67/300], Step [40/417], D Loss: 0.0000, G Loss: 36.7453\n",
      "Epoch [67/300], Step [60/417], D Loss: 0.0000, G Loss: 23.8713\n",
      "Epoch [67/300], Step [80/417], D Loss: 0.1043, G Loss: 52.2163\n",
      "Epoch [67/300], Step [100/417], D Loss: 0.0181, G Loss: 47.3592\n",
      "Epoch [67/300], Step [120/417], D Loss: 0.0114, G Loss: 11.8743\n",
      "Epoch [67/300], Step [140/417], D Loss: 0.0202, G Loss: 37.6963\n",
      "Epoch [67/300], Step [160/417], D Loss: 0.0180, G Loss: 7.2228\n",
      "Epoch [67/300], Step [180/417], D Loss: 0.0006, G Loss: 14.8436\n",
      "Epoch [67/300], Step [200/417], D Loss: 0.0022, G Loss: 12.5952\n",
      "Epoch [67/300], Step [220/417], D Loss: 0.8094, G Loss: 41.4851\n",
      "Epoch [67/300], Step [240/417], D Loss: 0.0035, G Loss: 17.7355\n",
      "Epoch [67/300], Step [260/417], D Loss: 0.0054, G Loss: 27.7602\n",
      "Epoch [67/300], Step [280/417], D Loss: 0.0057, G Loss: 24.6125\n",
      "Epoch [67/300], Step [300/417], D Loss: 0.1879, G Loss: 30.9092\n",
      "Epoch [67/300], Step [320/417], D Loss: 0.0004, G Loss: 11.9768\n",
      "Epoch [67/300], Step [340/417], D Loss: 0.3963, G Loss: 33.2594\n",
      "Epoch [67/300], Step [360/417], D Loss: 0.0012, G Loss: 13.1835\n",
      "Epoch [67/300], Step [380/417], D Loss: 0.0002, G Loss: 17.7621\n",
      "Epoch [67/300], Step [400/417], D Loss: 0.0025, G Loss: 5.5943\n",
      "Epoch [68/300], Step [20/417], D Loss: 0.0000, G Loss: 35.1590\n",
      "Epoch [68/300], Step [40/417], D Loss: 0.0000, G Loss: 21.0429\n",
      "Epoch [68/300], Step [60/417], D Loss: 0.0101, G Loss: 19.2223\n",
      "Epoch [68/300], Step [80/417], D Loss: 0.0000, G Loss: 17.0455\n",
      "Epoch [68/300], Step [100/417], D Loss: 0.0001, G Loss: 16.2405\n",
      "Epoch [68/300], Step [120/417], D Loss: 0.0002, G Loss: 35.0947\n",
      "Epoch [68/300], Step [140/417], D Loss: 0.0000, G Loss: 14.7945\n",
      "Epoch [68/300], Step [160/417], D Loss: 0.0021, G Loss: 12.4828\n",
      "Epoch [68/300], Step [180/417], D Loss: 0.0000, G Loss: 38.5919\n",
      "Epoch [68/300], Step [200/417], D Loss: 0.0002, G Loss: 28.3214\n",
      "Epoch [68/300], Step [220/417], D Loss: 0.0002, G Loss: 19.5877\n",
      "Epoch [68/300], Step [240/417], D Loss: 0.0000, G Loss: 39.6941\n",
      "Epoch [68/300], Step [260/417], D Loss: 0.0000, G Loss: 25.0332\n",
      "Epoch [68/300], Step [280/417], D Loss: 0.0001, G Loss: 14.3284\n",
      "Epoch [68/300], Step [300/417], D Loss: 0.0009, G Loss: 11.8912\n",
      "Epoch [68/300], Step [320/417], D Loss: 0.1535, G Loss: 26.1608\n",
      "Epoch [68/300], Step [340/417], D Loss: 0.0038, G Loss: 20.4661\n",
      "Epoch [68/300], Step [360/417], D Loss: 0.1141, G Loss: 31.9656\n",
      "Epoch [68/300], Step [380/417], D Loss: 0.0048, G Loss: 15.5098\n",
      "Epoch [68/300], Step [400/417], D Loss: 0.0006, G Loss: 7.0908\n",
      "Epoch [69/300], Step [20/417], D Loss: 0.2947, G Loss: 15.3601\n",
      "Epoch [69/300], Step [40/417], D Loss: 0.0726, G Loss: 30.4181\n",
      "Epoch [69/300], Step [60/417], D Loss: 1.2732, G Loss: 27.6120\n",
      "Epoch [69/300], Step [80/417], D Loss: 0.0183, G Loss: 8.9963\n",
      "Epoch [69/300], Step [100/417], D Loss: 0.0886, G Loss: 9.4394\n",
      "Epoch [69/300], Step [120/417], D Loss: 0.0133, G Loss: 20.1202\n",
      "Epoch [69/300], Step [140/417], D Loss: 0.0052, G Loss: 17.6364\n",
      "Epoch [69/300], Step [160/417], D Loss: 0.0012, G Loss: 15.2751\n",
      "Epoch [69/300], Step [180/417], D Loss: 0.0144, G Loss: 7.0223\n",
      "Epoch [69/300], Step [200/417], D Loss: 0.0021, G Loss: 7.7824\n",
      "Epoch [69/300], Step [220/417], D Loss: 0.1519, G Loss: 29.1909\n",
      "Epoch [69/300], Step [240/417], D Loss: 0.0001, G Loss: 16.4716\n",
      "Epoch [69/300], Step [260/417], D Loss: 0.0002, G Loss: 9.8391\n",
      "Epoch [69/300], Step [280/417], D Loss: 0.0007, G Loss: 6.3561\n",
      "Epoch [69/300], Step [300/417], D Loss: 0.0120, G Loss: 11.1703\n",
      "Epoch [69/300], Step [320/417], D Loss: 0.0943, G Loss: 10.8168\n",
      "Epoch [69/300], Step [340/417], D Loss: 0.0034, G Loss: 6.0397\n",
      "Epoch [69/300], Step [360/417], D Loss: 0.0001, G Loss: 28.6023\n",
      "Epoch [69/300], Step [380/417], D Loss: 0.0049, G Loss: 22.9073\n",
      "Epoch [69/300], Step [400/417], D Loss: 0.0014, G Loss: 27.7265\n",
      "Epoch [70/300], Step [20/417], D Loss: 0.0207, G Loss: 43.1932\n",
      "Epoch [70/300], Step [40/417], D Loss: 0.0003, G Loss: 38.7284\n",
      "Epoch [70/300], Step [60/417], D Loss: 0.0001, G Loss: 23.9412\n",
      "Epoch [70/300], Step [80/417], D Loss: 0.0059, G Loss: 23.1551\n",
      "Epoch [70/300], Step [100/417], D Loss: 2.8590, G Loss: 53.0793\n",
      "Epoch [70/300], Step [120/417], D Loss: 0.0007, G Loss: 53.4351\n",
      "Epoch [70/300], Step [140/417], D Loss: 0.0000, G Loss: 53.2954\n",
      "Epoch [70/300], Step [160/417], D Loss: 0.0000, G Loss: 50.9149\n",
      "Epoch [70/300], Step [180/417], D Loss: 0.0000, G Loss: 51.4093\n",
      "Epoch [70/300], Step [200/417], D Loss: 0.0000, G Loss: 51.4952\n",
      "Epoch [70/300], Step [220/417], D Loss: 0.0000, G Loss: 51.7013\n",
      "Epoch [70/300], Step [240/417], D Loss: 0.0000, G Loss: 51.1371\n",
      "Epoch [70/300], Step [260/417], D Loss: 0.0002, G Loss: 49.7723\n",
      "Epoch [70/300], Step [280/417], D Loss: 0.0000, G Loss: 48.9069\n",
      "Epoch [70/300], Step [300/417], D Loss: 0.0011, G Loss: 51.7529\n",
      "Epoch [70/300], Step [320/417], D Loss: 0.0000, G Loss: 51.4727\n",
      "Epoch [70/300], Step [340/417], D Loss: 0.0000, G Loss: 52.1557\n",
      "Epoch [70/300], Step [360/417], D Loss: 0.0007, G Loss: 50.2243\n",
      "Epoch [70/300], Step [380/417], D Loss: 0.0000, G Loss: 51.0078\n",
      "Epoch [70/300], Step [400/417], D Loss: 0.0000, G Loss: 50.0045\n",
      "Epoch [71/300], Step [20/417], D Loss: 0.0000, G Loss: 51.7469\n",
      "Epoch [71/300], Step [40/417], D Loss: 0.0000, G Loss: 50.1241\n",
      "Epoch [71/300], Step [60/417], D Loss: 0.0000, G Loss: 49.4789\n",
      "Epoch [71/300], Step [80/417], D Loss: 0.0000, G Loss: 50.6000\n",
      "Epoch [71/300], Step [100/417], D Loss: 0.0000, G Loss: 49.6837\n",
      "Epoch [71/300], Step [120/417], D Loss: 0.0000, G Loss: 49.2035\n",
      "Epoch [71/300], Step [140/417], D Loss: 0.0000, G Loss: 49.2442\n",
      "Epoch [71/300], Step [160/417], D Loss: 0.0000, G Loss: 49.6168\n",
      "Epoch [71/300], Step [180/417], D Loss: 0.0000, G Loss: 38.5875\n",
      "Epoch [71/300], Step [200/417], D Loss: 0.0007, G Loss: 10.5751\n",
      "Epoch [71/300], Step [220/417], D Loss: 0.0000, G Loss: 50.9621\n",
      "Epoch [71/300], Step [240/417], D Loss: 0.0008, G Loss: 48.4854\n",
      "Epoch [71/300], Step [260/417], D Loss: 0.0000, G Loss: 46.4828\n",
      "Epoch [71/300], Step [280/417], D Loss: 0.0000, G Loss: 48.1691\n",
      "Epoch [71/300], Step [300/417], D Loss: 0.0000, G Loss: 48.8202\n",
      "Epoch [71/300], Step [320/417], D Loss: 0.0000, G Loss: 49.3557\n",
      "Epoch [71/300], Step [340/417], D Loss: 0.0003, G Loss: 47.4312\n",
      "Epoch [71/300], Step [360/417], D Loss: 0.0000, G Loss: 46.4570\n",
      "Epoch [71/300], Step [380/417], D Loss: 0.0000, G Loss: 48.6697\n",
      "Epoch [71/300], Step [400/417], D Loss: 0.0000, G Loss: 50.1082\n",
      "Epoch [72/300], Step [20/417], D Loss: 0.0000, G Loss: 46.7834\n",
      "Epoch [72/300], Step [40/417], D Loss: 0.0000, G Loss: 49.4208\n",
      "Epoch [72/300], Step [60/417], D Loss: 0.0000, G Loss: 47.9882\n",
      "Epoch [72/300], Step [80/417], D Loss: 0.0000, G Loss: 52.0775\n",
      "Epoch [72/300], Step [100/417], D Loss: 0.0000, G Loss: 48.1098\n",
      "Epoch [72/300], Step [120/417], D Loss: 0.0000, G Loss: 50.6073\n",
      "Epoch [72/300], Step [140/417], D Loss: 0.0000, G Loss: 50.3370\n",
      "Epoch [72/300], Step [160/417], D Loss: 0.0000, G Loss: 49.6411\n",
      "Epoch [72/300], Step [180/417], D Loss: 0.0000, G Loss: 46.0320\n",
      "Epoch [72/300], Step [200/417], D Loss: 0.0000, G Loss: 48.0724\n",
      "Epoch [72/300], Step [220/417], D Loss: 0.0000, G Loss: 48.3079\n",
      "Epoch [72/300], Step [240/417], D Loss: 0.0000, G Loss: 49.0132\n",
      "Epoch [72/300], Step [260/417], D Loss: 0.0000, G Loss: 48.9951\n",
      "Epoch [72/300], Step [280/417], D Loss: 0.0000, G Loss: 48.4442\n",
      "Epoch [72/300], Step [300/417], D Loss: 0.0000, G Loss: 44.1733\n",
      "Epoch [72/300], Step [320/417], D Loss: 0.0000, G Loss: 21.5784\n",
      "Epoch [72/300], Step [340/417], D Loss: 0.0000, G Loss: 58.4713\n",
      "Epoch [72/300], Step [360/417], D Loss: 0.0000, G Loss: 57.6160\n",
      "Epoch [72/300], Step [380/417], D Loss: 0.0000, G Loss: 59.0617\n",
      "Epoch [72/300], Step [400/417], D Loss: 0.0000, G Loss: 58.1486\n",
      "Epoch [73/300], Step [20/417], D Loss: 0.0000, G Loss: 56.8823\n",
      "Epoch [73/300], Step [40/417], D Loss: 0.0000, G Loss: 57.8929\n",
      "Epoch [73/300], Step [60/417], D Loss: 0.0000, G Loss: 58.1151\n",
      "Epoch [73/300], Step [80/417], D Loss: 0.0000, G Loss: 57.6288\n",
      "Epoch [73/300], Step [100/417], D Loss: 0.0000, G Loss: 58.3440\n",
      "Epoch [73/300], Step [120/417], D Loss: 0.0000, G Loss: 57.8565\n",
      "Epoch [73/300], Step [140/417], D Loss: 0.0000, G Loss: 57.9161\n",
      "Epoch [73/300], Step [160/417], D Loss: 0.0000, G Loss: 58.3824\n",
      "Epoch [73/300], Step [180/417], D Loss: 0.0000, G Loss: 58.6037\n",
      "Epoch [73/300], Step [200/417], D Loss: 0.0000, G Loss: 57.6340\n",
      "Epoch [73/300], Step [220/417], D Loss: 0.0000, G Loss: 58.6504\n",
      "Epoch [73/300], Step [240/417], D Loss: 0.0000, G Loss: 58.1771\n",
      "Epoch [73/300], Step [260/417], D Loss: 0.0000, G Loss: 57.6872\n",
      "Epoch [73/300], Step [280/417], D Loss: 0.0000, G Loss: 57.3392\n",
      "Epoch [73/300], Step [300/417], D Loss: 0.0000, G Loss: 57.6366\n",
      "Epoch [73/300], Step [320/417], D Loss: 0.0000, G Loss: 57.6975\n",
      "Epoch [73/300], Step [340/417], D Loss: 0.0000, G Loss: 57.6227\n",
      "Epoch [73/300], Step [360/417], D Loss: 0.0000, G Loss: 57.4233\n",
      "Epoch [73/300], Step [380/417], D Loss: 0.0000, G Loss: 57.5478\n",
      "Epoch [73/300], Step [400/417], D Loss: 0.0000, G Loss: 57.4865\n",
      "Epoch [74/300], Step [20/417], D Loss: 0.0000, G Loss: 57.7391\n",
      "Epoch [74/300], Step [40/417], D Loss: 0.0000, G Loss: 57.6365\n",
      "Epoch [74/300], Step [60/417], D Loss: 0.0000, G Loss: 56.8251\n",
      "Epoch [74/300], Step [80/417], D Loss: 0.0000, G Loss: 57.7056\n",
      "Epoch [74/300], Step [100/417], D Loss: 0.0000, G Loss: 57.7083\n",
      "Epoch [74/300], Step [120/417], D Loss: 0.0000, G Loss: 57.4699\n",
      "Epoch [74/300], Step [140/417], D Loss: 0.0000, G Loss: 58.5300\n",
      "Epoch [74/300], Step [160/417], D Loss: 0.0000, G Loss: 57.9261\n",
      "Epoch [74/300], Step [180/417], D Loss: 0.0000, G Loss: 56.9178\n",
      "Epoch [74/300], Step [200/417], D Loss: 0.0000, G Loss: 57.0247\n",
      "Epoch [74/300], Step [220/417], D Loss: 0.0000, G Loss: 58.3561\n",
      "Epoch [74/300], Step [240/417], D Loss: 0.0000, G Loss: 57.8010\n",
      "Epoch [74/300], Step [260/417], D Loss: 0.0000, G Loss: 56.2165\n",
      "Epoch [74/300], Step [280/417], D Loss: 0.0000, G Loss: 58.1213\n",
      "Epoch [74/300], Step [300/417], D Loss: 0.0000, G Loss: 57.6225\n",
      "Epoch [74/300], Step [320/417], D Loss: 0.0000, G Loss: 58.0904\n",
      "Epoch [74/300], Step [340/417], D Loss: 0.0000, G Loss: 56.7320\n",
      "Epoch [74/300], Step [360/417], D Loss: 0.0000, G Loss: 56.8870\n",
      "Epoch [74/300], Step [380/417], D Loss: 0.0000, G Loss: 56.9025\n",
      "Epoch [74/300], Step [400/417], D Loss: 0.0000, G Loss: 55.6112\n",
      "Epoch [75/300], Step [20/417], D Loss: 0.0000, G Loss: 56.2635\n",
      "Epoch [75/300], Step [40/417], D Loss: 0.0000, G Loss: 57.1573\n",
      "Epoch [75/300], Step [60/417], D Loss: 0.0000, G Loss: 57.0605\n",
      "Epoch [75/300], Step [80/417], D Loss: 0.0000, G Loss: 55.9243\n",
      "Epoch [75/300], Step [100/417], D Loss: 0.0000, G Loss: 57.2538\n",
      "Epoch [75/300], Step [120/417], D Loss: 0.0000, G Loss: 57.7785\n",
      "Epoch [75/300], Step [140/417], D Loss: 0.0000, G Loss: 56.5817\n",
      "Epoch [75/300], Step [160/417], D Loss: 0.0000, G Loss: 56.9208\n",
      "Epoch [75/300], Step [180/417], D Loss: 0.0000, G Loss: 56.9163\n",
      "Epoch [75/300], Step [200/417], D Loss: 0.0000, G Loss: 56.6744\n",
      "Epoch [75/300], Step [220/417], D Loss: 0.0000, G Loss: 56.5974\n",
      "Epoch [75/300], Step [240/417], D Loss: 0.0000, G Loss: 56.3791\n",
      "Epoch [75/300], Step [260/417], D Loss: 0.0000, G Loss: 56.5447\n",
      "Epoch [75/300], Step [280/417], D Loss: 0.0000, G Loss: 56.7062\n",
      "Epoch [75/300], Step [300/417], D Loss: 0.0000, G Loss: 57.0180\n",
      "Epoch [75/300], Step [320/417], D Loss: 0.0000, G Loss: 56.5056\n",
      "Epoch [75/300], Step [340/417], D Loss: 0.0000, G Loss: 56.9821\n",
      "Epoch [75/300], Step [360/417], D Loss: 0.0000, G Loss: 56.9553\n",
      "Epoch [75/300], Step [380/417], D Loss: 0.0000, G Loss: 56.2896\n",
      "Epoch [75/300], Step [400/417], D Loss: 0.0000, G Loss: 56.5473\n",
      "Epoch [76/300], Step [20/417], D Loss: 0.0000, G Loss: 56.0603\n",
      "Epoch [76/300], Step [40/417], D Loss: 0.0000, G Loss: 55.8654\n",
      "Epoch [76/300], Step [60/417], D Loss: 0.0000, G Loss: 56.8256\n",
      "Epoch [76/300], Step [80/417], D Loss: 0.0000, G Loss: 56.4028\n",
      "Epoch [76/300], Step [100/417], D Loss: 0.0000, G Loss: 55.6692\n",
      "Epoch [76/300], Step [120/417], D Loss: 0.0000, G Loss: 56.7084\n",
      "Epoch [76/300], Step [140/417], D Loss: 0.0000, G Loss: 56.4021\n",
      "Epoch [76/300], Step [160/417], D Loss: 0.0000, G Loss: 55.5653\n",
      "Epoch [76/300], Step [180/417], D Loss: 0.0000, G Loss: 56.3306\n",
      "Epoch [76/300], Step [200/417], D Loss: 0.0000, G Loss: 56.2887\n",
      "Epoch [76/300], Step [220/417], D Loss: 0.0000, G Loss: 55.6679\n",
      "Epoch [76/300], Step [240/417], D Loss: 0.0000, G Loss: 56.6805\n",
      "Epoch [76/300], Step [260/417], D Loss: 0.0000, G Loss: 55.5240\n",
      "Epoch [76/300], Step [280/417], D Loss: 0.0000, G Loss: 55.5313\n",
      "Epoch [76/300], Step [300/417], D Loss: 0.0000, G Loss: 55.9856\n",
      "Epoch [76/300], Step [320/417], D Loss: 0.0000, G Loss: 55.6505\n",
      "Epoch [76/300], Step [340/417], D Loss: 0.0000, G Loss: 55.0532\n",
      "Epoch [76/300], Step [360/417], D Loss: 0.0000, G Loss: 53.8983\n",
      "Epoch [76/300], Step [380/417], D Loss: 0.0000, G Loss: 53.0906\n",
      "Epoch [76/300], Step [400/417], D Loss: 0.0000, G Loss: 53.1365\n",
      "Epoch [77/300], Step [20/417], D Loss: 0.0000, G Loss: 54.0719\n",
      "Epoch [77/300], Step [40/417], D Loss: 0.0000, G Loss: 53.9720\n",
      "Epoch [77/300], Step [60/417], D Loss: 0.0000, G Loss: 53.7763\n",
      "Epoch [77/300], Step [80/417], D Loss: 0.0000, G Loss: 54.4898\n",
      "Epoch [77/300], Step [100/417], D Loss: 0.0000, G Loss: 53.8913\n",
      "Epoch [77/300], Step [120/417], D Loss: 0.0000, G Loss: 53.7019\n",
      "Epoch [77/300], Step [140/417], D Loss: 0.0000, G Loss: 53.8647\n",
      "Epoch [77/300], Step [160/417], D Loss: 0.0000, G Loss: 54.1710\n",
      "Epoch [77/300], Step [180/417], D Loss: 0.0000, G Loss: 53.7846\n",
      "Epoch [77/300], Step [200/417], D Loss: 0.0000, G Loss: 53.7034\n",
      "Epoch [77/300], Step [220/417], D Loss: 0.0000, G Loss: 53.6273\n",
      "Epoch [77/300], Step [240/417], D Loss: 0.0000, G Loss: 53.8662\n",
      "Epoch [77/300], Step [260/417], D Loss: 0.0000, G Loss: 53.4854\n",
      "Epoch [77/300], Step [280/417], D Loss: 0.0000, G Loss: 53.5697\n",
      "Epoch [77/300], Step [300/417], D Loss: 0.0000, G Loss: 54.0158\n",
      "Epoch [77/300], Step [320/417], D Loss: 0.0000, G Loss: 53.5817\n",
      "Epoch [77/300], Step [340/417], D Loss: 0.0000, G Loss: 54.2597\n",
      "Epoch [77/300], Step [360/417], D Loss: 0.0000, G Loss: 54.2664\n",
      "Epoch [77/300], Step [380/417], D Loss: 0.0000, G Loss: 53.4820\n",
      "Epoch [77/300], Step [400/417], D Loss: 0.0000, G Loss: 53.6628\n",
      "Epoch [78/300], Step [20/417], D Loss: 0.0000, G Loss: 53.4953\n",
      "Epoch [78/300], Step [40/417], D Loss: 0.0000, G Loss: 53.4589\n",
      "Epoch [78/300], Step [60/417], D Loss: 0.0000, G Loss: 53.9318\n",
      "Epoch [78/300], Step [80/417], D Loss: 1.7791, G Loss: 40.4968\n",
      "Epoch [78/300], Step [100/417], D Loss: 0.0000, G Loss: 35.8591\n",
      "Epoch [78/300], Step [120/417], D Loss: 0.0000, G Loss: 24.2466\n",
      "Epoch [78/300], Step [140/417], D Loss: 0.0000, G Loss: 57.2565\n",
      "Epoch [78/300], Step [160/417], D Loss: 0.0000, G Loss: 55.6772\n",
      "Epoch [78/300], Step [180/417], D Loss: 0.0000, G Loss: 57.9159\n",
      "Epoch [78/300], Step [200/417], D Loss: 0.0001, G Loss: 58.1211\n",
      "Epoch [78/300], Step [220/417], D Loss: 0.0000, G Loss: 54.9323\n",
      "Epoch [78/300], Step [240/417], D Loss: 0.0000, G Loss: 54.7596\n",
      "Epoch [78/300], Step [260/417], D Loss: 0.0001, G Loss: 56.3348\n",
      "Epoch [78/300], Step [280/417], D Loss: 0.0001, G Loss: 54.7862\n",
      "Epoch [78/300], Step [300/417], D Loss: 0.0000, G Loss: 56.0181\n",
      "Epoch [78/300], Step [320/417], D Loss: 0.0000, G Loss: 53.0363\n",
      "Epoch [78/300], Step [340/417], D Loss: 0.0003, G Loss: 57.3310\n",
      "Epoch [78/300], Step [360/417], D Loss: 0.0000, G Loss: 53.7102\n",
      "Epoch [78/300], Step [380/417], D Loss: 0.0000, G Loss: 56.4376\n",
      "Epoch [78/300], Step [400/417], D Loss: 0.0000, G Loss: 56.7172\n",
      "Epoch [79/300], Step [20/417], D Loss: 0.0000, G Loss: 54.3736\n",
      "Epoch [79/300], Step [40/417], D Loss: 0.0000, G Loss: 51.7416\n",
      "Epoch [79/300], Step [60/417], D Loss: 0.0000, G Loss: 55.1301\n",
      "Epoch [79/300], Step [80/417], D Loss: 0.0000, G Loss: 56.4179\n",
      "Epoch [79/300], Step [100/417], D Loss: 0.0000, G Loss: 53.9456\n",
      "Epoch [79/300], Step [120/417], D Loss: 0.0000, G Loss: 51.3100\n",
      "Epoch [79/300], Step [140/417], D Loss: 0.0000, G Loss: 54.6628\n",
      "Epoch [79/300], Step [160/417], D Loss: 0.0000, G Loss: 53.9408\n",
      "Epoch [79/300], Step [180/417], D Loss: 0.0000, G Loss: 54.5959\n",
      "Epoch [79/300], Step [200/417], D Loss: 0.0000, G Loss: 53.5684\n",
      "Epoch [79/300], Step [220/417], D Loss: 0.0000, G Loss: 50.3815\n",
      "Epoch [79/300], Step [240/417], D Loss: 0.0000, G Loss: 55.0574\n",
      "Epoch [79/300], Step [260/417], D Loss: 0.0000, G Loss: 54.0888\n",
      "Epoch [79/300], Step [280/417], D Loss: 0.0000, G Loss: 55.0448\n",
      "Epoch [79/300], Step [300/417], D Loss: 0.0000, G Loss: 51.1582\n",
      "Epoch [79/300], Step [320/417], D Loss: 0.0000, G Loss: 54.4614\n",
      "Epoch [79/300], Step [340/417], D Loss: 0.0000, G Loss: 50.3203\n",
      "Epoch [79/300], Step [360/417], D Loss: 0.0000, G Loss: 51.3708\n",
      "Epoch [79/300], Step [380/417], D Loss: 0.0000, G Loss: 54.7969\n",
      "Epoch [79/300], Step [400/417], D Loss: 0.0000, G Loss: 54.7022\n",
      "Epoch [80/300], Step [20/417], D Loss: 0.0000, G Loss: 56.0974\n",
      "Epoch [80/300], Step [40/417], D Loss: 0.0000, G Loss: 50.6712\n",
      "Epoch [80/300], Step [60/417], D Loss: 0.0000, G Loss: 54.6633\n",
      "Epoch [80/300], Step [80/417], D Loss: 0.0000, G Loss: 52.7799\n",
      "Epoch [80/300], Step [100/417], D Loss: 0.0000, G Loss: 50.0789\n",
      "Epoch [80/300], Step [120/417], D Loss: 0.0000, G Loss: 54.9557\n",
      "Epoch [80/300], Step [140/417], D Loss: 0.0000, G Loss: 53.0252\n",
      "Epoch [80/300], Step [160/417], D Loss: 0.0000, G Loss: 55.4976\n",
      "Epoch [80/300], Step [180/417], D Loss: 0.0000, G Loss: 53.9208\n",
      "Epoch [80/300], Step [200/417], D Loss: 0.0000, G Loss: 53.6366\n",
      "Epoch [80/300], Step [220/417], D Loss: 0.0000, G Loss: 54.7549\n",
      "Epoch [80/300], Step [240/417], D Loss: 0.0000, G Loss: 53.4431\n",
      "Epoch [80/300], Step [260/417], D Loss: 0.0000, G Loss: 51.8025\n",
      "Epoch [80/300], Step [280/417], D Loss: 0.0000, G Loss: 53.3526\n",
      "Epoch [80/300], Step [300/417], D Loss: 0.0000, G Loss: 49.8316\n",
      "Epoch [80/300], Step [320/417], D Loss: 0.0000, G Loss: 50.4726\n",
      "Epoch [80/300], Step [340/417], D Loss: 0.0000, G Loss: 54.3305\n",
      "Epoch [80/300], Step [360/417], D Loss: 0.0000, G Loss: 52.8349\n",
      "Epoch [80/300], Step [380/417], D Loss: 0.0000, G Loss: 53.3318\n",
      "Epoch [80/300], Step [400/417], D Loss: 0.0000, G Loss: 53.2949\n",
      "Epoch [81/300], Step [20/417], D Loss: 0.0000, G Loss: 48.8499\n",
      "Epoch [81/300], Step [40/417], D Loss: 0.0000, G Loss: 53.1266\n",
      "Epoch [81/300], Step [60/417], D Loss: 0.0000, G Loss: 52.8504\n",
      "Epoch [81/300], Step [80/417], D Loss: 0.0000, G Loss: 53.4449\n",
      "Epoch [81/300], Step [100/417], D Loss: 0.0000, G Loss: 50.5980\n",
      "Epoch [81/300], Step [120/417], D Loss: 0.0000, G Loss: 49.7982\n",
      "Epoch [81/300], Step [140/417], D Loss: 0.0000, G Loss: 52.2489\n",
      "Epoch [81/300], Step [160/417], D Loss: 0.0000, G Loss: 54.8312\n",
      "Epoch [81/300], Step [180/417], D Loss: 0.0000, G Loss: 52.5921\n",
      "Epoch [81/300], Step [200/417], D Loss: 0.0000, G Loss: 52.5990\n",
      "Epoch [81/300], Step [220/417], D Loss: 0.0000, G Loss: 52.3361\n",
      "Epoch [81/300], Step [240/417], D Loss: 0.0000, G Loss: 54.7574\n",
      "Epoch [81/300], Step [260/417], D Loss: 0.0000, G Loss: 53.2231\n",
      "Epoch [81/300], Step [280/417], D Loss: 0.0000, G Loss: 54.1555\n",
      "Epoch [81/300], Step [300/417], D Loss: 0.0000, G Loss: 49.4550\n",
      "Epoch [81/300], Step [320/417], D Loss: 0.0000, G Loss: 52.8081\n",
      "Epoch [81/300], Step [340/417], D Loss: 0.0000, G Loss: 49.7276\n",
      "Epoch [81/300], Step [360/417], D Loss: 0.0000, G Loss: 50.6076\n",
      "Epoch [81/300], Step [380/417], D Loss: 0.0000, G Loss: 52.4907\n",
      "Epoch [81/300], Step [400/417], D Loss: 0.0000, G Loss: 51.4506\n",
      "Epoch [82/300], Step [20/417], D Loss: 0.0000, G Loss: 52.5162\n",
      "Epoch [82/300], Step [40/417], D Loss: 0.0000, G Loss: 47.3365\n",
      "Epoch [82/300], Step [60/417], D Loss: 0.0000, G Loss: 49.3923\n",
      "Epoch [82/300], Step [80/417], D Loss: 0.0000, G Loss: 50.7773\n",
      "Epoch [82/300], Step [100/417], D Loss: 0.0000, G Loss: 46.0413\n",
      "Epoch [82/300], Step [120/417], D Loss: 0.0000, G Loss: 57.7921\n",
      "Epoch [82/300], Step [140/417], D Loss: 0.0009, G Loss: 34.2999\n",
      "Epoch [82/300], Step [160/417], D Loss: 0.0000, G Loss: 10.8484\n",
      "Epoch [82/300], Step [180/417], D Loss: 0.0533, G Loss: 11.9326\n",
      "Epoch [82/300], Step [200/417], D Loss: 2.2384, G Loss: 8.4343\n",
      "Epoch [82/300], Step [220/417], D Loss: 0.0482, G Loss: 18.0301\n",
      "Epoch [82/300], Step [240/417], D Loss: 0.0000, G Loss: 24.0018\n",
      "Epoch [82/300], Step [260/417], D Loss: 0.0197, G Loss: 6.2319\n",
      "Epoch [82/300], Step [280/417], D Loss: 0.0059, G Loss: 11.6027\n",
      "Epoch [82/300], Step [300/417], D Loss: 0.0224, G Loss: 36.9191\n",
      "Epoch [82/300], Step [320/417], D Loss: 0.0002, G Loss: 13.5257\n",
      "Epoch [82/300], Step [340/417], D Loss: 0.0001, G Loss: 8.4048\n",
      "Epoch [82/300], Step [360/417], D Loss: 0.0000, G Loss: 12.7512\n",
      "Epoch [82/300], Step [380/417], D Loss: 0.0219, G Loss: 30.7420\n",
      "Epoch [82/300], Step [400/417], D Loss: 0.0000, G Loss: 19.7191\n",
      "Epoch [83/300], Step [20/417], D Loss: 0.0066, G Loss: 9.6937\n",
      "Epoch [83/300], Step [40/417], D Loss: 0.0004, G Loss: 22.3028\n",
      "Epoch [83/300], Step [60/417], D Loss: 0.3201, G Loss: 9.0414\n",
      "Epoch [83/300], Step [80/417], D Loss: 0.0004, G Loss: 9.2810\n",
      "Epoch [83/300], Step [100/417], D Loss: 0.0083, G Loss: 14.5131\n",
      "Epoch [83/300], Step [120/417], D Loss: 0.0199, G Loss: 7.3630\n",
      "Epoch [83/300], Step [140/417], D Loss: 0.0011, G Loss: 13.2035\n",
      "Epoch [83/300], Step [160/417], D Loss: 0.0023, G Loss: 11.0256\n",
      "Epoch [83/300], Step [180/417], D Loss: 0.0092, G Loss: 6.9673\n",
      "Epoch [83/300], Step [200/417], D Loss: 0.2146, G Loss: 32.3670\n",
      "Epoch [83/300], Step [220/417], D Loss: 0.0001, G Loss: 14.0779\n",
      "Epoch [83/300], Step [240/417], D Loss: 0.0000, G Loss: 27.9440\n",
      "Epoch [83/300], Step [260/417], D Loss: 0.0001, G Loss: 9.5791\n",
      "Epoch [83/300], Step [280/417], D Loss: 0.0007, G Loss: 24.9925\n",
      "Epoch [83/300], Step [300/417], D Loss: 0.0000, G Loss: 29.4058\n",
      "Epoch [83/300], Step [320/417], D Loss: 0.0009, G Loss: 25.4369\n",
      "Epoch [83/300], Step [340/417], D Loss: 0.0002, G Loss: 11.0832\n",
      "Epoch [83/300], Step [360/417], D Loss: 0.0061, G Loss: 15.9889\n",
      "Epoch [83/300], Step [380/417], D Loss: 3.7762, G Loss: 28.0374\n",
      "Epoch [83/300], Step [400/417], D Loss: 0.0003, G Loss: 27.7079\n",
      "Epoch [84/300], Step [20/417], D Loss: 0.0051, G Loss: 28.4308\n",
      "Epoch [84/300], Step [40/417], D Loss: 0.0000, G Loss: 28.6891\n",
      "Epoch [84/300], Step [60/417], D Loss: 0.0000, G Loss: 12.4186\n",
      "Epoch [84/300], Step [80/417], D Loss: 0.0010, G Loss: 8.5139\n",
      "Epoch [84/300], Step [100/417], D Loss: 0.0005, G Loss: 15.9800\n",
      "Epoch [84/300], Step [120/417], D Loss: 0.0193, G Loss: 30.1118\n",
      "Epoch [84/300], Step [140/417], D Loss: 0.0573, G Loss: 18.2470\n",
      "Epoch [84/300], Step [160/417], D Loss: 0.0179, G Loss: 29.1237\n",
      "Epoch [84/300], Step [180/417], D Loss: 0.0646, G Loss: 21.6566\n",
      "Epoch [84/300], Step [200/417], D Loss: 0.0000, G Loss: 17.8585\n",
      "Epoch [84/300], Step [220/417], D Loss: 0.0001, G Loss: 15.9702\n",
      "Epoch [84/300], Step [240/417], D Loss: 0.0003, G Loss: 29.3678\n",
      "Epoch [84/300], Step [260/417], D Loss: 0.0001, G Loss: 20.4541\n",
      "Epoch [84/300], Step [280/417], D Loss: 0.0001, G Loss: 21.2401\n",
      "Epoch [84/300], Step [300/417], D Loss: 0.0426, G Loss: 20.0236\n",
      "Epoch [84/300], Step [320/417], D Loss: 0.0002, G Loss: 26.6218\n",
      "Epoch [84/300], Step [340/417], D Loss: 0.0142, G Loss: 12.7705\n",
      "Epoch [84/300], Step [360/417], D Loss: 0.0000, G Loss: 16.4287\n",
      "Epoch [84/300], Step [380/417], D Loss: 0.0009, G Loss: 9.0192\n",
      "Epoch [84/300], Step [400/417], D Loss: 1.4413, G Loss: 28.0807\n",
      "Epoch [85/300], Step [20/417], D Loss: 0.0014, G Loss: 14.6018\n",
      "Epoch [85/300], Step [40/417], D Loss: 0.0000, G Loss: 28.3783\n",
      "Epoch [85/300], Step [60/417], D Loss: 0.0005, G Loss: 19.7773\n",
      "Epoch [85/300], Step [80/417], D Loss: 0.0026, G Loss: 13.1991\n",
      "Epoch [85/300], Step [100/417], D Loss: 0.0051, G Loss: 25.1591\n",
      "Epoch [85/300], Step [120/417], D Loss: 0.0022, G Loss: 31.7327\n",
      "Epoch [85/300], Step [140/417], D Loss: 0.0000, G Loss: 34.9585\n",
      "Epoch [85/300], Step [160/417], D Loss: 0.0002, G Loss: 37.8013\n",
      "Epoch [85/300], Step [180/417], D Loss: 0.0010, G Loss: 21.0066\n",
      "Epoch [85/300], Step [200/417], D Loss: 0.0000, G Loss: 15.6174\n",
      "Epoch [85/300], Step [220/417], D Loss: 0.6608, G Loss: 27.1166\n",
      "Epoch [85/300], Step [240/417], D Loss: 0.0001, G Loss: 24.5982\n",
      "Epoch [85/300], Step [260/417], D Loss: 0.0005, G Loss: 20.5414\n",
      "Epoch [85/300], Step [280/417], D Loss: 0.0000, G Loss: 24.5646\n",
      "Epoch [85/300], Step [300/417], D Loss: 0.0063, G Loss: 20.2224\n",
      "Epoch [85/300], Step [320/417], D Loss: 0.0007, G Loss: 3.8030\n",
      "Epoch [85/300], Step [340/417], D Loss: 0.0002, G Loss: 21.1831\n",
      "Epoch [85/300], Step [360/417], D Loss: 0.0003, G Loss: 16.7497\n",
      "Epoch [85/300], Step [380/417], D Loss: 0.0006, G Loss: 41.6685\n",
      "Epoch [85/300], Step [400/417], D Loss: 0.0000, G Loss: 36.6913\n",
      "Epoch [86/300], Step [20/417], D Loss: 0.0001, G Loss: 14.7641\n",
      "Epoch [86/300], Step [40/417], D Loss: 0.0027, G Loss: 27.0691\n",
      "Epoch [86/300], Step [60/417], D Loss: 0.0034, G Loss: 23.9580\n",
      "Epoch [86/300], Step [80/417], D Loss: 0.0000, G Loss: 28.4694\n",
      "Epoch [86/300], Step [100/417], D Loss: 0.0002, G Loss: 23.4646\n",
      "Epoch [86/300], Step [120/417], D Loss: 0.0001, G Loss: 14.3679\n",
      "Epoch [86/300], Step [140/417], D Loss: 0.1516, G Loss: 29.7339\n",
      "Epoch [86/300], Step [160/417], D Loss: 0.0389, G Loss: 19.4526\n",
      "Epoch [86/300], Step [180/417], D Loss: 0.0005, G Loss: 15.9837\n",
      "Epoch [86/300], Step [200/417], D Loss: 0.0042, G Loss: 15.4711\n",
      "Epoch [86/300], Step [220/417], D Loss: 0.0000, G Loss: 24.7222\n",
      "Epoch [86/300], Step [240/417], D Loss: 0.0005, G Loss: 16.1210\n",
      "Epoch [86/300], Step [260/417], D Loss: 0.0166, G Loss: 46.3041\n",
      "Epoch [86/300], Step [280/417], D Loss: 0.0002, G Loss: 41.9937\n",
      "Epoch [86/300], Step [300/417], D Loss: 0.0000, G Loss: 27.5427\n",
      "Epoch [86/300], Step [320/417], D Loss: 0.0007, G Loss: 15.7938\n",
      "Epoch [86/300], Step [340/417], D Loss: 0.0002, G Loss: 5.2031\n",
      "Epoch [86/300], Step [360/417], D Loss: 0.0080, G Loss: 4.9606\n",
      "Epoch [86/300], Step [380/417], D Loss: 0.0000, G Loss: 11.2663\n",
      "Epoch [86/300], Step [400/417], D Loss: 0.1038, G Loss: 13.9070\n",
      "Epoch [87/300], Step [20/417], D Loss: 0.1333, G Loss: 16.1893\n",
      "Epoch [87/300], Step [40/417], D Loss: 0.0006, G Loss: 18.6662\n",
      "Epoch [87/300], Step [60/417], D Loss: 0.0000, G Loss: 21.7326\n",
      "Epoch [87/300], Step [80/417], D Loss: 0.0004, G Loss: 9.5663\n",
      "Epoch [87/300], Step [100/417], D Loss: 0.0133, G Loss: 20.6596\n",
      "Epoch [87/300], Step [120/417], D Loss: 0.0001, G Loss: 21.6807\n",
      "Epoch [87/300], Step [140/417], D Loss: 0.0000, G Loss: 28.3077\n",
      "Epoch [87/300], Step [160/417], D Loss: 0.0000, G Loss: 19.2860\n",
      "Epoch [87/300], Step [180/417], D Loss: 0.0385, G Loss: 16.3493\n",
      "Epoch [87/300], Step [200/417], D Loss: 0.0017, G Loss: 18.2725\n",
      "Epoch [87/300], Step [220/417], D Loss: 0.0351, G Loss: 18.8392\n",
      "Epoch [87/300], Step [240/417], D Loss: 0.6751, G Loss: 33.6054\n",
      "Epoch [87/300], Step [260/417], D Loss: 0.0049, G Loss: 16.0495\n",
      "Epoch [87/300], Step [280/417], D Loss: 0.0000, G Loss: 26.7322\n",
      "Epoch [87/300], Step [300/417], D Loss: 0.0002, G Loss: 31.8134\n",
      "Epoch [87/300], Step [320/417], D Loss: 0.0000, G Loss: 18.6604\n",
      "Epoch [87/300], Step [340/417], D Loss: 0.0007, G Loss: 47.3454\n",
      "Epoch [87/300], Step [360/417], D Loss: 0.0000, G Loss: 47.3264\n",
      "Epoch [87/300], Step [380/417], D Loss: 0.0005, G Loss: 43.1163\n",
      "Epoch [87/300], Step [400/417], D Loss: 0.0000, G Loss: 38.4465\n",
      "Epoch [88/300], Step [20/417], D Loss: 0.1151, G Loss: 40.5874\n",
      "Epoch [88/300], Step [40/417], D Loss: 0.0000, G Loss: 23.0987\n",
      "Epoch [88/300], Step [60/417], D Loss: 2.0407, G Loss: 57.0517\n",
      "Epoch [88/300], Step [80/417], D Loss: 0.0001, G Loss: 49.7966\n",
      "Epoch [88/300], Step [100/417], D Loss: 0.0017, G Loss: 54.2510\n",
      "Epoch [88/300], Step [120/417], D Loss: 0.0000, G Loss: 49.2889\n",
      "Epoch [88/300], Step [140/417], D Loss: 0.0001, G Loss: 49.3617\n",
      "Epoch [88/300], Step [160/417], D Loss: 0.0000, G Loss: 41.5318\n",
      "Epoch [88/300], Step [180/417], D Loss: 0.0001, G Loss: 60.6698\n",
      "Epoch [88/300], Step [200/417], D Loss: 0.0001, G Loss: 61.8883\n",
      "Epoch [88/300], Step [220/417], D Loss: 0.0004, G Loss: 58.9200\n",
      "Epoch [88/300], Step [240/417], D Loss: 0.0000, G Loss: 59.5210\n",
      "Epoch [88/300], Step [260/417], D Loss: 0.0000, G Loss: 59.2570\n",
      "Epoch [88/300], Step [280/417], D Loss: 0.0000, G Loss: 57.9709\n",
      "Epoch [88/300], Step [300/417], D Loss: 0.0001, G Loss: 56.3266\n",
      "Epoch [88/300], Step [320/417], D Loss: 0.0000, G Loss: 59.5401\n",
      "Epoch [88/300], Step [340/417], D Loss: 0.0000, G Loss: 57.4182\n",
      "Epoch [88/300], Step [360/417], D Loss: 0.0003, G Loss: 58.6349\n",
      "Epoch [88/300], Step [380/417], D Loss: 0.0000, G Loss: 57.5159\n",
      "Epoch [88/300], Step [400/417], D Loss: 0.0000, G Loss: 57.7082\n",
      "Epoch [89/300], Step [20/417], D Loss: 0.0000, G Loss: 56.2529\n",
      "Epoch [89/300], Step [40/417], D Loss: 0.0000, G Loss: 57.1771\n",
      "Epoch [89/300], Step [60/417], D Loss: 0.0000, G Loss: 57.4340\n",
      "Epoch [89/300], Step [80/417], D Loss: 0.0000, G Loss: 56.7458\n",
      "Epoch [89/300], Step [100/417], D Loss: 0.0000, G Loss: 56.7045\n",
      "Epoch [89/300], Step [120/417], D Loss: 0.0000, G Loss: 58.9176\n",
      "Epoch [89/300], Step [140/417], D Loss: 0.0000, G Loss: 57.4342\n",
      "Epoch [89/300], Step [160/417], D Loss: 0.0000, G Loss: 55.5164\n",
      "Epoch [89/300], Step [180/417], D Loss: 0.0000, G Loss: 57.3795\n",
      "Epoch [89/300], Step [200/417], D Loss: 0.0000, G Loss: 55.6363\n",
      "Epoch [89/300], Step [220/417], D Loss: 0.0000, G Loss: 55.6568\n",
      "Epoch [89/300], Step [240/417], D Loss: 0.0000, G Loss: 55.3399\n",
      "Epoch [89/300], Step [260/417], D Loss: 0.0000, G Loss: 56.4161\n",
      "Epoch [89/300], Step [280/417], D Loss: 0.0000, G Loss: 53.1183\n",
      "Epoch [89/300], Step [300/417], D Loss: 0.0000, G Loss: 54.6065\n",
      "Epoch [89/300], Step [320/417], D Loss: 0.0000, G Loss: 49.5340\n",
      "Epoch [89/300], Step [340/417], D Loss: 0.0000, G Loss: 41.8237\n",
      "Epoch [89/300], Step [360/417], D Loss: 0.0000, G Loss: 41.9232\n",
      "Epoch [89/300], Step [380/417], D Loss: 0.0000, G Loss: 41.6677\n",
      "Epoch [89/300], Step [400/417], D Loss: 0.0000, G Loss: 41.8590\n",
      "Epoch [90/300], Step [20/417], D Loss: 0.0000, G Loss: 41.5140\n",
      "Epoch [90/300], Step [40/417], D Loss: 0.0000, G Loss: 41.7545\n",
      "Epoch [90/300], Step [60/417], D Loss: 0.0000, G Loss: 41.1522\n",
      "Epoch [90/300], Step [80/417], D Loss: 0.0000, G Loss: 43.1698\n",
      "Epoch [90/300], Step [100/417], D Loss: 0.0000, G Loss: 42.9963\n",
      "Epoch [90/300], Step [120/417], D Loss: 0.0000, G Loss: 41.5025\n",
      "Epoch [90/300], Step [140/417], D Loss: 0.0000, G Loss: 41.5820\n",
      "Epoch [90/300], Step [160/417], D Loss: 0.0000, G Loss: 42.0527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[105], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (real_images, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m----> 4\u001B[0m         real_images \u001B[38;5;241m=\u001B[39m \u001B[43mreal_images\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m         real_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(real_images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m      6\u001B[0m         fake_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(real_images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x0000023391F81360> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(0, num_epochs):\n",
    "    try:\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            real_images = real_images.to(device)\n",
    "            real_labels = torch.ones(real_images.size(0), device=device)\n",
    "            fake_labels = torch.zeros(real_images.size(0), device=device)\n",
    "        \n",
    "            ### Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_outputs = discriminator(real_images)\n",
    "            d_loss_real = criterion(real_outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "        \n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            fake_outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            ### Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            # Optionally regenerate fake images for freshness\n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            output = discriminator(fake_images)\n",
    "        \n",
    "            # Randomly decide whether to flip labels\n",
    "            if random.random() < flip_prob:\n",
    "                g_loss = criterion(output, fake_labels)  # Flipped labels\n",
    "            else:\n",
    "                g_loss = criterion(output, real_labels)  # Normal training\n",
    "        \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    \n",
    "            if (i + 1) % 20 == 0:\n",
    "                d_losses.append(d_loss_real.item() + d_loss_fake.item())\n",
    "                g_losses.append(g_loss.item())\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss_real.item() + d_loss_fake.item():.4f}, '\n",
    "                      f'G Loss: {g_loss.item():.4f}'\n",
    "                      f'; D Loss Real: {d_loss_real.item():.4f}, '\n",
    "                      f'; D Loss Fake: {d_loss_fake.item():.4f}, '\n",
    "                      )\n",
    "                \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            # check_output(fake_images[0], epoch)\n",
    "            generate_and_plot_images(25, epoch=epoch, plot=False)\n",
    "            \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            checkpoint = {\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch  # Optional, if you want to also save the epoch number\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'GAN_cat_{epoch}.pth')\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"An error occurred while processing the image. Epoch: {epoch}, batch: {i}\")\n",
    "        continue\n"
   ],
   "id": "6701c6aa62fd5790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming 'generator' and 'discriminator' are your model instances\n",
    "# And 'optimizer_G' and 'optimizer_D' are the optimizers for the generator and discriminator respectively\n",
    "\n",
    "# Define checkpoint dictionary\n",
    "checkpoint = {\n",
    "    'generator_state_dict': generator.state_dict(),\n",
    "    'discriminator_state_dict': discriminator.state_dict(),\n",
    "    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "    'epoch': epoch  # Optional, if you want to also save the epoch number\n",
    "}\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(checkpoint, f'GAN_checkpoint_main_128_{epoch}.pth')\n"
   ],
   "id": "61435395198a0aab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_and_plot_images(25, epoch=1000, plot=True)",
   "id": "3743b1d77bd20c28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# check_output(real_images[-1])",
   "id": "1ccea9e8dc667c8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(1301, 1320):\n",
    "    generate_and_plot_images(25, epoch=1301)"
   ],
   "id": "4473c5676c5d0d54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate random noise\n",
    "noise = torch.randn(1, 100, 1, 1, device=device)  # Batch size of 1, latent vector size of 100\n",
    "\n",
    "# Generate an image\n",
    "with torch.no_grad():  # Temporarily set all the requires_grad flags to false\n",
    "    generated_image = generator(noise)"
   ],
   "id": "6f1d62115679dfb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# generated_image = fake_images[23]\n",
    "\n",
    "generated_image = generated_image.to('cpu').clone().detach()  # Move to CPU and detach from the computation graph\n",
    "generated_image = generated_image.numpy()  # Convert to numpy array\n",
    "generated_image = generated_image.squeeze(0)  # Remove the batch dimension, resulting in (channels, height, width)\n",
    "\n",
    "# If the image is in the format (C, H, W), convert it to (H, W, C)\n",
    "if generated_image.shape[0] == 3:  # Assuming 3 channels for RGB\n",
    "    generated_image = generated_image.transpose(1, 2, 0)  # Reorder dimensions to (H, W, C)\n",
    "\n",
    "# Normalize the image to [0, 1] if it's not already\n",
    "generated_image = (generated_image + 1) / 2  # Assuming that the output is in the range [-1, 1]\n",
    "generated_image = generated_image.clip(0, 1)  # Ensure the values are within [0, 1]\n"
   ],
   "id": "7474c75710241e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(generated_image)\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n"
   ],
   "id": "6478769c57d136c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for num, generated_image in enumerate(fake_images):\n",
    "    # Generate random noise\n",
    "    noise = torch.randn(1, 100, 1, 1, device=device) / 100  # Batch size of 1, latent vector size of 100\n",
    "    \n",
    "    # Generate an image\n",
    "    with torch.no_grad():  # Temporarily set all the requires_grad flags to false\n",
    "        generated_image = generator(noise)\n",
    "\n",
    "    generated_image = generated_image.to('cpu').clone().detach()  # Move to CPU and detach from the computation graph\n",
    "    generated_image = generated_image.numpy()  # Convert to numpy array\n",
    "    generated_image = generated_image.squeeze(0)  # Remove the batch dimension, resulting in (channels, height, width)\n",
    "    \n",
    "    # If the image is in the format (C, H, W), convert it to (H, W, C)\n",
    "    if generated_image.shape[0] == 3:  # Assuming 3 channels for RGB\n",
    "        generated_image = generated_image.transpose(1, 2, 0)  # Reorder dimensions to (H, W, C)\n",
    "    \n",
    "    # Normalize the image to [0, 1] if it's not already\n",
    "    generated_image = (generated_image + 1) / 2  # Assuming that the output is in the range [-1, 1]\n",
    "    generated_image = generated_image.clip(0, 1)  # Ensure the values are within [0, 1]\n",
    "\n",
    "    \n",
    "    plt.imshow(generated_image)\n",
    "    plt.savefig(f'output/generated_imag_{num}.png')\n"
   ],
   "id": "fe35cfd1257bde73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for num, generated_image in enumerate(fake_images):\n",
    "\n",
    "    generated_image = generated_image.to('cpu').clone().detach()  # Move to CPU and detach from the computation graph\n",
    "    generated_image = generated_image.numpy()  # Convert to numpy array\n",
    "    # generated_image = generated_image.squeeze(0)  # Remove the batch dimension, resulting in (channels, height, width)\n",
    "    \n",
    "    # If the image is in the format (C, H, W), convert it to (H, W, C)\n",
    "    if generated_image.shape[0] == 3:  # Assuming 3 channels for RGB\n",
    "        generated_image = generated_image.transpose(1, 2, 0)  # Reorder dimensions to (H, W, C)\n",
    "    \n",
    "    # Normalize the image to [0, 1] if it's not already\n",
    "    generated_image = (generated_image + 1) / 2  # Assuming that the output is in the range [-1, 1]\n",
    "    generated_image = generated_image.clip(0, 1)  # Ensure the values are within [0, 1]\n",
    "    \n",
    "    plt.imshow(generated_image)\n",
    "    plt.savefig(f'output/fake_imag_{num}.png')\n"
   ],
   "id": "363cd17dd99562dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for num, real_image in enumerate(real_images):\n",
    "\n",
    "    real_image = real_image.to('cpu').clone().detach()  # Move to CPU and detach from the computation graph\n",
    "    real_image = real_image.numpy()  # Convert to numpy array\n",
    "    # generated_image = generated_image.squeeze(0)  # Remove the batch dimension, resulting in (channels, height, width)\n",
    "    \n",
    "    # If the image is in the format (C, H, W), convert it to (H, W, C)\n",
    "    if real_image.shape[0] == 3:  # Assuming 3 channels for RGB\n",
    "        real_image = real_image.transpose(1, 2, 0)  # Reorder dimensions to (H, W, C)\n",
    "    \n",
    "    # Normalize the image to [0, 1] if it's not already\n",
    "    real_image = (real_image + 1) / 2  # Assuming that the output is in the range [-1, 1]\n",
    "    real_image = real_image.clip(0, 1)  # Ensure the values are within [0, 1]\n",
    "    \n",
    "    plt.imshow(real_image)\n",
    "    break\n"
   ],
   "id": "55c3ee86b9f35826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e5d66a6b98a15bb1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
