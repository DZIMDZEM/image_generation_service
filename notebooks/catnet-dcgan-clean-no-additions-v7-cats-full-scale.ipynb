{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:17.033645Z",
     "start_time": "2024-06-03T18:58:14.825511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import random"
   ],
   "id": "b10883462b527219",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "341e1487ebb71811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:17.049054Z",
     "start_time": "2024-06-03T18:58:17.036792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your dataset\n",
    "dataset_path = r\"C:\\Users\\dzmit\\Downloads\\cat_images\\cats_64x64\""
   ],
   "id": "3f10b3d19164ed8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:17.360182Z",
     "start_time": "2024-06-03T18:58:17.051422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Resize all images to 64x64\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "# dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ],
   "id": "ebdc6330df45d27b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:17.375573Z",
     "start_time": "2024-06-03T18:58:17.362418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoader\n",
    "batch_size = 128  # Batch size"
   ],
   "id": "63731794a1fcc00",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:17.422356Z",
     "start_time": "2024-06-03T18:58:17.406923Z"
    }
   },
   "cell_type": "code",
   "source": "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)",
   "id": "460c215c96b56ded",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:19.167688Z",
     "start_time": "2024-06-03T18:58:19.149063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, ngf, output_channels_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(latent_size, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, output_channels_size, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (output_channels_size) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ],
   "id": "9cd0e609b0d9e763",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:20.243951Z",
     "start_time": "2024-06-03T18:58:20.225201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_channels_size, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (output_channels_size) x 64 x 64\n",
    "            nn.Conv2d(output_channels_size, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ],
   "id": "b4ec03cf704289bd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Latent vector",
   "id": "a09e7c8e0ab3fbd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:21.938901Z",
     "start_time": "2024-06-03T18:58:21.933382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_noise(size):\n",
    "    # Generate random noise directly with specified mean and std\n",
    "    center = 0\n",
    "    std = 0.2 \n",
    "    \n",
    "    return torch.normal(center, std, size=(size, latent_size, 1, 1)).to(device)"
   ],
   "id": "c40166c5f755c46f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:22.454538Z",
     "start_time": "2024-06-03T18:58:22.431685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def sample_spherical(npoints, ndim=3):\n",
    "    vec = np.random.randn(ndim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    return vec\n",
    "\n",
    "def interpolate_along_great_circle(latent_dim, num_steps, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates a series of latent vectors interpolated along a great circle in the latent space.\n",
    "    \n",
    "    Parameters:\n",
    "        latent_dim (int): Dimension of the latent space.\n",
    "        num_steps (int): Number of interpolation steps along the great circle.\n",
    "        device (str): Device to which the latent vectors will be sent ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Interpolated latent vectors shaped (num_steps, latent_dim, 1, 1).\n",
    "    \"\"\"\n",
    "    # Generate two points on the unit sphere in latent space\n",
    "    points = torch.tensor(sample_spherical(2, latent_dim), dtype=torch.float, device=device).t()\n",
    "    start, end = points[0], points[1]\n",
    "    \n",
    "    # Compute the angle between them\n",
    "    dot = torch.dot(start, end)\n",
    "    theta = torch.acos(dot)\n",
    "    \n",
    "    # Generate the steps\n",
    "    steps = torch.linspace(0, 1, num_steps, device=device)\n",
    "    sin_t = torch.sin(theta)\n",
    "    \n",
    "    # Perform the interpolation\n",
    "    latent_vectors = []\n",
    "    for step in steps:\n",
    "        alpha = torch.sin((1 - step) * theta) / sin_t\n",
    "        beta = torch.sin(step * theta) / sin_t\n",
    "        interpolated_point = alpha * start + beta * end\n",
    "        latent_vectors.append(interpolated_point.unsqueeze(0).unsqueeze(-1).unsqueeze(-1))\n",
    "    \n",
    "    # Concatenate all interpolated points\n",
    "    return torch.cat(latent_vectors, dim=0)\n"
   ],
   "id": "1bb75c552d860282",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "a4e1e6a4a9dbc453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:24.356389Z",
     "start_time": "2024-06-03T18:58:24.324097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ],
   "id": "a2c6b74b77c58978",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:24.919684Z",
     "start_time": "2024-06-03T18:58:24.892029Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "e193b77c5f2fc5b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the model",
   "id": "c06399e5245aed9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:27.009858Z",
     "start_time": "2024-06-03T18:58:26.995276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latent_size = 100\n",
    "ngf = 64\n",
    "output_channels_size = 3\n",
    "ndf = 64"
   ],
   "id": "269286cf391eb06f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:27.912741Z",
     "start_time": "2024-06-03T18:58:27.750758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = Generator(latent_size, ngf, output_channels_size).to(device)\n",
    "discriminator = Discriminator(output_channels_size, ndf).to(device)"
   ],
   "id": "b2f05555c8f75738",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:40.859794Z",
     "start_time": "2024-06-03T18:58:28.831727Z"
    }
   },
   "cell_type": "code",
   "source": "discriminator(torch.randn(1, 3, 64, 64, device=device))",
   "id": "65d184e53af4329a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4539]]]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:46:51.399063Z",
     "start_time": "2024-05-22T18:46:51.382063Z"
    }
   },
   "cell_type": "code",
   "source": "create_noise(3).shape",
   "id": "dfb2fdc51f5a06a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:46:41.393954Z",
     "start_time": "2024-05-22T18:46:41.382710Z"
    }
   },
   "cell_type": "code",
   "source": "generator(create_noise(3)).shape",
   "id": "cb2a8da9065df6fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 64, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:46:58.948983Z",
     "start_time": "2024-05-22T18:46:58.934982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "num_steps = 10  # Number of interpolation steps\n",
    "latent_vectors = interpolate_along_great_circle(latent_size, 3, device)"
   ],
   "id": "ee5a1ce7b0fa3d69",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:46:59.357072Z",
     "start_time": "2024-05-22T18:46:59.347073Z"
    }
   },
   "cell_type": "code",
   "source": "latent_vectors.shape",
   "id": "80f2ecf15f04fc24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T18:47:00.233883Z",
     "start_time": "2024-05-22T18:47:00.224340Z"
    }
   },
   "cell_type": "code",
   "source": "generator(latent_vectors).shape",
   "id": "3b44ff3fffe3230f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 64, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optimizers",
   "id": "17e57702f9cb05ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:58:40.890516Z",
     "start_time": "2024-06-03T18:58:40.867409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ],
   "id": "79e3ca6230018c26",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the checkpoint",
   "id": "f1522e71b0d8009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T07:44:57.988013Z",
     "start_time": "2024-05-22T07:44:57.980115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checkpoint = torch.load('GAN_cat_299.pth')\n",
    "# \n",
    "# # Assuming the generator and discriminator are already instantiated as per the saved model architecture\n",
    "# generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "# discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "# \n",
    "# # Assuming the optimizers are already instantiated with the parameters of their respective models\n",
    "# optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "# optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "# \n",
    "# # If you saved the epoch number, you can also load this to know where to resume training\n",
    "# epoch = checkpoint['epoch']\n"
   ],
   "id": "45b9a74ae7638170",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T07:44:58.003201Z",
     "start_time": "2024-05-22T07:44:57.993573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for param_group in optimizer_G.param_groups:\n",
    "#     param_group['lr'] *= 0.1\n",
    "# \n",
    "# for param_group in optimizer_D.param_groups:\n",
    "#     param_group['lr'] *= 0.1"
   ],
   "id": "a2ec4a4d62a148e0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plotting functions",
   "id": "b8dda8fae9cd518b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:59:24.064260Z",
     "start_time": "2024-06-03T18:59:24.047144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def save_images(fake_images, epoch, prefix='gan', folder='generated_images_cats_themselves', n_images=25):\n",
    "    \"\"\"\n",
    "    Saves a grid of generated images to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - fake_images: Tensor of images generated by the GAN.\n",
    "    - epoch: Current epoch number, used for naming the output file.\n",
    "    - prefix: Prefix string for the filename.\n",
    "    - folder: Output directory for saving the images.\n",
    "    - n_images: Number of images to save. Default is 25.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # Select the first n_images from the batch\n",
    "    images_to_save = fake_images[:n_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 10))  # Setting up a 5x5 grid\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, img in enumerate(images_to_save):\n",
    "        img = img.to('cpu').detach().numpy()  # Convert tensor to numpy array\n",
    "        # if img.shape[0] == 3:  # If there are 3 channels (RGB)\n",
    "        img = img.transpose(1, 2, 0)  # Change from CxHxW to HxWxC\n",
    "        # else:\n",
    "        #     img = img.squeeze(0)  # If grayscale, remove channel dimension\n",
    "        \n",
    "        # Normalize image to [0, 1]\n",
    "        img = (img + 1) / 2\n",
    "        img = img.clip(0, 1)  # Ensure pixel values are within the [0, 1] range\n",
    "        \n",
    "        axes[idx].imshow(img, cmap='gray')\n",
    "        axes[idx].axis('off')  # Hide axes to enhance visual appeal\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Construct the filename using the prefix and epoch\n",
    "    filename = f\"{prefix}_epoch_{epoch}.png\"\n",
    "    plt.savefig(os.path.join(folder, filename))\n",
    "    plt.close(fig)  # Close the plot to free memory\n",
    "\n",
    "# Example usage (assuming `fake_images` is your batch of generated images and `epoch` is your current epoch):\n",
    "# save_images(fake_images, epoch, prefix='myGAN', folder='my_images')\n"
   ],
   "id": "bf027ac48f09730b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:59:24.792488Z",
     "start_time": "2024-06-03T18:59:24.783390Z"
    }
   },
   "cell_type": "code",
   "source": "# generate_and_plot_images(n_images=25, epoch=200)",
   "id": "e26c9972161b12e2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clear cache",
   "id": "bb958409548b2dab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T19:51:50.813084Z",
     "start_time": "2024-06-03T19:51:50.516107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import gc\n",
    "# \n",
    "# torch.cuda.empty_cache()  # Clear cache\n",
    "# gc.collect()  # Collect garbage\n",
    "# generator.to('cpu')\n",
    "# discriminator.to('cpu')\n",
    "# \n",
    "# del generator, discriminator, optimizer_G, optimizer_D"
   ],
   "id": "db4be72ff0c55575",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:59:35.191875Z",
     "start_time": "2024-06-03T18:59:35.184876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 300\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []"
   ],
   "id": "11a1971908ad53d7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T18:59:37.594224Z",
     "start_time": "2024-06-03T18:59:35.954955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'generator' and 'discriminator' are your models\n",
    "# 'optimizer_G' and 'optimizer_D' are the respective optimizers\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        for batch_number, (real_images, _) in enumerate(dataloader):\n",
    "    \n",
    "            real_images = real_images.to(device)\n",
    "            \n",
    "            batch_size = real_images.size(0)\n",
    "            # Add tiny random noise around 0.9 for the real labels\n",
    "            noise_epsilon = 0.01  # Standard deviation of noise\n",
    "            real_labels = torch.full((batch_size,), 0.9, device=device) + torch.randn(batch_size, device=device) * noise_epsilon\n",
    "            fake_labels = torch.zeros(batch_size, device=device) + torch.randn(batch_size, device=device) * noise_epsilon\n",
    "    \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            outputs_real = discriminator(real_images)\n",
    "            loss_real = -torch.mean(torch.log(outputs_real + 1e-8))\n",
    "            loss_real.backward()\n",
    "    \n",
    "            # noise = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "            latent_vector = interpolate_along_great_circle(latent_size, batch_size, device)\n",
    "\n",
    "            fake_images = generator(latent_vector)\n",
    "            \n",
    "            outputs_fake = discriminator(fake_images.detach())\n",
    "            loss_fake = -torch.mean(torch.log(1 - outputs_fake + 1e-8))\n",
    "            loss_fake.backward()\n",
    "            optimizer_D.step()\n",
    "    \n",
    "            # Train Generator with possible label flipping\n",
    "            optimizer_G.zero_grad()\n",
    "            # Randomly decide whether to flip labels\n",
    "            flip = np.random.rand() < 0.1  # 10% chance to flip labels\n",
    "            if flip:\n",
    "                # Train generator to produce 'fake' labeled as 'real', but use 'fake' label\n",
    "                gen_labels = fake_labels\n",
    "            else:\n",
    "                # Normal training, train generator to produce 'fake' labeled as 'real'\n",
    "                gen_labels = real_labels\n",
    "    \n",
    "            outputs_fake_for_gen = discriminator(fake_images).squeeze()\n",
    "            # print(outputs_fake_for_gen.shape, gen_labels.shape)\n",
    "            loss_G = criterion(outputs_fake_for_gen, gen_labels)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Logging and validation here (if applicable)\n",
    "            if (batch_number + 1) % 20 == 0:\n",
    "                d_losses.append(loss_real.item() + loss_fake.item())\n",
    "                g_losses.append(loss_G.item())\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_number + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {loss_real.item() + loss_fake.item():.4f}, '\n",
    "                      f'G Loss: {loss_G.item():.4f}'\n",
    "                      f'; D Loss Real: {loss_real.item():.4f}, '\n",
    "                      f'; D Loss Fake: {loss_fake.item():.4f}, '\n",
    "                      )\n",
    "    \n",
    "        # if (epoch + 1) % 1 == 0:\n",
    "            # check_output(fake_images[0], epoch)\n",
    "        save_images(fake_images, epoch, prefix='gan', folder='generated_images', n_images=25)\n",
    "            \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint = {\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch  # Optional, if you want to also save the epoch number\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'dcgan_cat_{epoch}.pth')\n",
    "            \n",
    "    except OSError:\n",
    "        print(f\"An error occurred while processing the image. Epoch: {epoch}, batch: {batch_number}\")\n",
    "        continue"
   ],
   "id": "702f6ea99ea50073",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to find a valid cuDNN algorithm to run convolution",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m outputs_real \u001B[38;5;241m=\u001B[39m discriminator(real_images)\n\u001B[0;32m     24\u001B[0m loss_real \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmean(torch\u001B[38;5;241m.\u001B[39mlog(outputs_real \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-8\u001B[39m))\n\u001B[1;32m---> 25\u001B[0m \u001B[43mloss_real\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# noise = torch.randn(batch_size, latent_size, 1, 1, device=device)\u001B[39;00m\n\u001B[0;32m     28\u001B[0m latent_vector \u001B[38;5;241m=\u001B[39m interpolate_along_great_circle(latent_size, batch_size, device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\kautra\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Unable to find a valid cuDNN algorithm to run convolution"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:49:39.910416Z",
     "start_time": "2024-05-22T20:49:39.893891Z"
    }
   },
   "cell_type": "code",
   "source": "fake_images.shape",
   "id": "511314ac550caa2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 3, 64, 64])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:50:23.030628Z",
     "start_time": "2024-05-22T20:50:23.013603Z"
    }
   },
   "cell_type": "code",
   "source": "real_images.shape",
   "id": "e15ebebb6e19af35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 3, 64, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_and_plot_images(n_images=9, epoch=0, plot=True):\n",
    "    \"\"\"\n",
    "    Generates and plots a grid of images using a trained generator model.\n",
    "\n",
    "    Parameters:\n",
    "    - generator: The trained generator model for generating images.\n",
    "    - device: The device (e.g., 'cuda' or 'cpu') the model should run on.\n",
    "    - n_images: The total number of images to generate and plot. Default is 9.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(9, 9))  # Create a 3x3 grid of subplots\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration\n",
    "\n",
    "    for i in range(n_images):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn(1, 256, 1, 1, device=device) / 100\n",
    "\n",
    "        # Generate an image without updating gradients\n",
    "        with torch.no_grad():\n",
    "            generated_image = generator(noise)\n",
    "\n",
    "        # Process the image for visualization\n",
    "        generated_image = generated_image.to('cpu').clone().detach()\n",
    "        generated_image = generated_image.numpy().squeeze(0)\n",
    "\n",
    "        if generated_image.shape[0] == 3:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.transpose(1, 2, 0)  # Convert from CxHxW to HxWxC\n",
    "            \n",
    "        elif generated_image.shape[0] == 1:  # Check if the image has 3 channels (RGB)\n",
    "            generated_image = generated_image.squeeze(0)  # Convert from CxHxW to HxWxC\n",
    "\n",
    "        # Normalize the image data to [0, 1]\n",
    "        generated_image = (generated_image + 1) / 2\n",
    "        generated_image = generated_image.clip(0, 1)  # Ensure pixel values are within the expected range\n",
    "\n",
    "        axes[i].imshow(generated_image, cmap='gray')\n",
    "        axes[i].axis('off')  # Turn off the axis to make the images look cleaner\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/generated_images_grid_{epoch}.png')\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n"
   ],
   "id": "cce66bfde454b0f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T07:45:12.971386Z",
     "start_time": "2024-05-22T07:45:04.332888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            real_images = real_images.to(device)\n",
    "            \n",
    "            # Labels for your batches\n",
    "            real_labels = torch.full((batch_size,), 0.9, device=device)  # Real labels smoothed to 0.9\n",
    "            # real_labels = torch.ones(real_images.size(0), device=device)\n",
    "            fake_labels = torch.zeros(real_images.size(0), device=device)\n",
    "        \n",
    "            ### Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_outputs = discriminator(real_images)\n",
    "            d_loss_real = criterion(real_outputs, real_labels)\n",
    "            d_loss_real.backward()\n",
    "        \n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            fake_outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            d_loss_fake.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "            ### Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            # Optionally regenerate fake images for freshness\n",
    "            noise = torch.randn(real_images.size(0), latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(noise)\n",
    "            output = discriminator(fake_images)\n",
    "        \n",
    "            # Randomly decide whether to flip labels\n",
    "            if random.random() < flip_prob:\n",
    "                g_loss = criterion(output, fake_labels)  # Flipped labels\n",
    "            else:\n",
    "                g_loss = criterion(output, real_labels)  # Normal training\n",
    "        \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    \n",
    "            if (i + 1) % 20 == 0:\n",
    "                d_losses.append(d_loss_real.item() + d_loss_fake.item())\n",
    "                g_losses.append(g_loss.item())\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(dataloader)}], '\n",
    "                      f'D Loss: {d_loss_real.item() + d_loss_fake.item():.4f}, '\n",
    "                      f'G Loss: {g_loss.item():.4f}'\n",
    "                      f'; D Loss Real: {d_loss_real.item():.4f}, '\n",
    "                      f'; D Loss Fake: {d_loss_fake.item():.4f}, '\n",
    "                      )\n",
    "                \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            # check_output(fake_images[0], epoch)\n",
    "            generate_and_plot_images(25, epoch=epoch, plot=False)\n",
    "            \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            checkpoint = {\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'epoch': epoch  # Optional, if you want to also save the epoch number\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'GAN_cat_{epoch}.pth')\n",
    "\n",
    "    except OSError:\n",
    "        print(f\"An error occurred while processing the image. Epoch: {epoch}, batch: {i}\")\n",
    "        continue\n"
   ],
   "id": "6701c6aa62fd5790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [20/313], D Loss: 2.7489, G Loss: 5.7095; D Loss Real: 2.7414, ; D Loss Fake: 0.0075, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e5d66a6b98a15bb1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
